{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text embeddings with Word2Vec, Sentence2Vec and Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many machine learning models require numeric features, thus text must be converted to numeric before they can be used in the model. Whilst a straight forward way is to one-hot encode the text, this results in extremely sparse datasets and an explosion in the number of columns. Text embedding is a smarter way to convert text to numeric features. We will look at Word2Vec, Sentence2Vec and Doc2Vec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encoding\n",
    "\n",
    "To better understand why word embeddings are useful, we start by looking at how one hot encoding works. Suppose we have the sentence \"The cat sat on the mat\". There are 5 unique words in this sentence, thus after one hot encoding, it would look like the matrix below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![one_hot_encode](one_hot_encode.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With just 5 words in our vocabularly, we have 5 new columns. Not only is this sparse, there are also no meaning between the numeric representation of words, as they are all just vectors with all 0s, except a 1 for the column mapped to the word. It would be better if we could compress and represent words as more densely filled numeric features, where the numeric word features could be compared against each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec\n",
    "\n",
    "How can we map words to meaningful numeric vectors? What do we mean when we intuitively think of 2 words as similar? The subtleties of language can not be captured in any simple way, however word2vec takes two approaches which seems to work well in practice:\n",
    "\n",
    "1. skip-gram approach - given a word, we try to predict its surrounding words\n",
    "2. Continuous bag of words (CBOW) approach - given surrounding words, predict the word\n",
    "\n",
    "In both cases, intuitively we are saying that the context of the word (i.e. words surrounding the given word) should be similar if two words are similar. The beauty of this approach is that we do need to provide any labels. Given any text, our labels are simply the surrounding words (or the word itself in case of CBOW).\n",
    "\n",
    "Word2Vec is actually just a simple neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![neural_net_word2vec](neural_net_word2vec.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the above diagram, we can see 3 layers:\n",
    "\n",
    "1. First, we have a one-hot vector of all the unique words in the vocabulary. The length of the vector is the number of unique words.\n",
    "2. In the middle, we have the hidden layer. The length of this vector will be the size of our embedded numeric representation of each feature.\n",
    "3. Lastly, we have an output layer. This is the same length as the input layer with each node being a word.\n",
    "\n",
    "In the case of skip-gram, the input is just a word. The label for this input will be the word before and after it (depending on window size). For example, for the sentence 'the cat sat on the mat', suppose we have window size 1. Given the input 'cat', there would be two targets: 'the' and 'sat'.\n",
    "\n",
    "That is all there really is to it. We have an input and output. There is no activation function in the hidden layer, and the output layer is just a softmax layer. You may ask, 'ok, but where is the embedding for each word?'. For each input word, there is a connection to each node in the hidden layer. In the diagram above, each input node is connected to 300 nodes in the hidden layer. Thus, there will be 300 weights from each input node to the nodes in the hidden layer. These 300 weights are the embedding for the words! For each input node, we have a different set of 300 weights.\n",
    "\n",
    "CBOW is very similar, with only the input and output effectively being swapped. This is clearly shown in the image below (with window size 2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![cbow_vs_skipgram](cbow_vs_skipgram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python implementation of Word2vec\n",
    "\n",
    "We will no have a go at implementing this in Python. We will use a dataset which contains text from different news articles. We will generate word embeddings for each word from this text, using gensim package.\n",
    "\n",
    "Let us read in this data and have a look at a few rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>As U.S. budget fight looms, Republicans flip t...</td>\n",
       "      <td>WASHINGTON (Reuters) - The head of a conservat...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U.S. military to accept transgender recruits o...</td>\n",
       "      <td>WASHINGTON (Reuters) - Transgender people will...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 29, 2017</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior U.S. Republican senator: 'Let Mr. Muell...</td>\n",
       "      <td>WASHINGTON (Reuters) - The special counsel inv...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FBI Russia probe helped by Australian diplomat...</td>\n",
       "      <td>WASHINGTON (Reuters) - Trump campaign adviser ...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 30, 2017</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trump wants Postal Service to charge 'much mor...</td>\n",
       "      <td>SEATTLE/WASHINGTON (Reuters) - President Donal...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 29, 2017</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  As U.S. budget fight looms, Republicans flip t...   \n",
       "1  U.S. military to accept transgender recruits o...   \n",
       "2  Senior U.S. Republican senator: 'Let Mr. Muell...   \n",
       "3  FBI Russia probe helped by Australian diplomat...   \n",
       "4  Trump wants Postal Service to charge 'much mor...   \n",
       "\n",
       "                                                text       subject  \\\n",
       "0  WASHINGTON (Reuters) - The head of a conservat...  politicsNews   \n",
       "1  WASHINGTON (Reuters) - Transgender people will...  politicsNews   \n",
       "2  WASHINGTON (Reuters) - The special counsel inv...  politicsNews   \n",
       "3  WASHINGTON (Reuters) - Trump campaign adviser ...  politicsNews   \n",
       "4  SEATTLE/WASHINGTON (Reuters) - President Donal...  politicsNews   \n",
       "\n",
       "                 date label  \n",
       "0  December 31, 2017   True  \n",
       "1  December 29, 2017   True  \n",
       "2  December 31, 2017   True  \n",
       "3  December 30, 2017   True  \n",
       "4  December 29, 2017   True  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in data\n",
    "true = pd.read_csv(\"True.csv\")\n",
    "fake = pd.read_csv(\"Fake.csv\")\n",
    "\n",
    "# assign target for true or fake news\n",
    "true['label'] = 'True'\n",
    "fake['label'] = 'Fake'\n",
    "\n",
    "df = pd.concat([true, fake], axis=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each news, we have the title, the text in the article, as well as subject, date and label. For creating the word embeddings, we will just use the text column.\n",
    "\n",
    "The gensim Word2Vec model requires the input to be a list of lists. Each item in the list is a 'document' and each document is also a list of tokenised words. We will iterate through each row, and use gensim's 'tokenize' function, which tokenises the article with some basic text cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate list of tokenized articles\n",
    "sent = [list(tokenize(row, lowercase=True)) for row in df['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build doc2vec model\n",
    "word2vec_model = Word2Vec(\n",
    "    sent\n",
    "    ,min_count=1\n",
    "    ,size= 50\n",
    "    ,workers=8\n",
    "    ,window =3\n",
    "    ,sg = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.197617  , -0.09111612,  0.536869  ,  0.5075222 , -0.20479237,\n",
       "       -0.12949395, -0.8391654 ,  0.34476176, -0.58287984, -0.2688873 ,\n",
       "       -0.5941344 ,  0.31438622,  0.14963606, -0.22333175,  0.36819482,\n",
       "        0.16479246, -0.41388914,  0.20203425,  0.13636146, -0.65705323,\n",
       "       -0.56397605,  0.23625855,  0.5943967 , -0.06735351,  0.00571863,\n",
       "        0.00537719,  0.45918745, -0.35381147, -0.8086109 , -0.07890741,\n",
       "        0.22426248,  0.29491538, -0.4335018 ,  0.05501224,  1.0677358 ,\n",
       "        0.21837595,  0.32808843, -0.03840185,  0.617614  , -0.20691352,\n",
       "       -0.4464837 , -0.47698906,  0.26200438, -0.939531  ,  0.18013324,\n",
       "        0.1676601 , -0.66991186, -0.29150403,  0.6372676 , -0.191315  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.wv['trump']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
