{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e745f09-60fe-49a5-ab7e-1062530d6590",
   "metadata": {},
   "source": [
    "# LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405ae33b-921c-4099-ac99-5ed07c71f339",
   "metadata": {},
   "source": [
    "Large Language Models (LLM) have become increasingly used in more and more complex applications. When we use LLM in applications such as chatbots, there are many things that are happening that need to be considered:\n",
    "- How do we keep track of historical context?\n",
    "- Can we simulate a conversation with different people / agents?\n",
    "- Can we direct the conversation to different 'people' depending on what is asked? (e.g. asking a panel of experts, and picking the most relevant expert based on the question)\n",
    "\n",
    "We need a framework that allows us to abstract away many of these complexities, and ideally make it easy to logically structure how we use these LLMs. Introducing LangChain:\n",
    "\n",
    "LangChain is an open-source framework for developing applications powered by LLM. This framework includes the ability to:\n",
    "- Efficiently integrate with popular AI platforms such as OpenAI (company behind ChatGPT) and Hugging Face\n",
    "- Connecting language driven models to data sources\n",
    "- Enable LLMs to interact dynamically with their environment\n",
    "\n",
    "It is designed to have modular components, that when combined together, can be used in many different applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15965ef8-7c1c-4154-89e4-3e1f91027efa",
   "metadata": {},
   "source": [
    "## Course outline\n",
    "- Models, Prompts and Output Parsers\n",
    "    - Calling OpenAI\n",
    "    - OpenAI Endpoints\n",
    "    - Prompt templates\n",
    "    - Using LangChain\n",
    "- Handling memory\n",
    "    - ConversationBufferMemory\n",
    "    - How do LLM store memory?\n",
    "    - ConversationBufferWindowMemory\n",
    "    - ConversationTokenBufferMemory\n",
    "    - ConverastionSummaryBufferMemory\n",
    "    - Other memory methods\n",
    "- Chains\n",
    "    - What is a chain?\n",
    "    - LLMChain\n",
    "    - SimpleSequentialChain\n",
    "    - SequentialChain\n",
    "    - RouterChain\n",
    "    - Other chains to explore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414125ee-3b49-4701-8f48-63c3e8d713e9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Models, Prompts and Output Parsers\n",
    "\n",
    "### Calling OpenAI\n",
    "\n",
    "Before we look into LangChain and what it can do, we will make direct calls to OpenAI to show you what LLMs can do.\n",
    "\n",
    "Let's look at an example to see how this works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de870e72-b2e6-4c52-b725-d2e189c982e4",
   "metadata": {},
   "source": [
    "If want to follow along, you will need your own OpenAI API key.\n",
    "\n",
    "Follow this link to get your own API key:\n",
    "https://platform.openai.com/account/api-keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a0f1a8b-fbdb-478e-8de6-306e0524ef52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install python-dotenv\n",
    "# !pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "916e64a2-4630-4415-8d5f-e328d7757f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your API key here\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "639fa87c-21e5-4f3b-91be-c92ac9224afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66038294-e2a3-4288-a9d9-507fd449d744",
   "metadata": {},
   "source": [
    "### OpenAI Endpoints\n",
    "\n",
    "An API (application programming interface) is a software intermediary that allows applications to talk to each other. An API endpoint is a specific location within an API that accepts requests and sends responses back.\n",
    "\n",
    "OpenAI has several API endpoints that are offered. This includes tasks such as:\n",
    "- Audio files to text\n",
    "- Chat responses given list of messages\n",
    "- Predicted text completion\n",
    "- Create vector embeddings given a text input\n",
    "- Generate images given prompts and image\n",
    "- and many more...\n",
    "\n",
    "It is fascinating how simple it is now to access all this through a simple API call.\n",
    "\n",
    "We will demonstrate using openAI's ChatCompletion API. This is an API that is useful for having conversations, where given a list of messages comprising a conversation, it will return a response (think like a Chatbot). Let's start by keeping it simple, and just demonstrate what it looks like to call this API endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5dbc1d-c8d1-41cc-8a61-c341350b049d",
   "metadata": {},
   "source": [
    "Let us choose ChatGPT as the LLM for this demo. We account for the deprecation of the LLM by comparing to the target date of June 12th 2024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7a3ed6e1-0b51-4c61-9a25-a55d3f09ecf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# account for deprecation of LLM model\n",
    "import datetime\n",
    "\n",
    "current_date = datetime.datetime.now().date() # Get the current date\n",
    "target_date = datetime.date(2024, 6, 12) # Define the date after which the model should be set to \"gpt-3.5-turbo\"\n",
    "\n",
    "# Set the model variable based on the current date\n",
    "if current_date > target_date:\n",
    "    llm_model = \"gpt-3.5-turbo\"\n",
    "else:\n",
    "    llm_model = \"gpt-3.5-turbo-0301\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663434e7-2b59-4a8e-977f-31ca63144224",
   "metadata": {},
   "source": [
    "We will now define a function that will take in a prompt, feed it to our chosen LLM, then return the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "095c773d-1572-4b8a-8e16-8c484c010050",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=llm_model):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0, \n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ab3da822-d3d1-4411-a9b2-985f4e58e72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this converations, we have different roles (e.g. a customer, and an assistant), and also a 'system' role, which you can think of background information that defines who the assistant is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "590e8f91-d7a6-4ccb-aa8f-65e16fc05167",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of Australia is Canberra.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_completion(prompt=\"What is the capital of Australia?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba40591-6fcb-4a04-bdae-2009607c528f",
   "metadata": {},
   "source": [
    "Here, we simply called the ChatCompletion API endpoint with a simple prompt, and got a response. A further breakdown of the function:\n",
    "- messages : this is the list of messages we are going to pass to the ChatCompletion API endpoint\n",
    "- response : this is the API endpoint, that takes the messages as input, uses the GPT-3.5 model to process and return a response\n",
    "- temperature : this is a parameter that defines how random the response should be. 0 is telling the model to be more deterministic.\n",
    "\n",
    "Let's now explore what else we can do with this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0df964-ff5c-4975-9001-450ef823fb73",
   "metadata": {},
   "source": [
    "### Prompt templates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558bfe92-7c2a-44d9-8259-1be3f47aba26",
   "metadata": {},
   "source": [
    "Let's now customise the prompt/input, so that it is more dynamic, and like a template we can reuse.\n",
    "\n",
    "Suppose we have a customer review of a restaurant they went to. It is written in a very rude tone. We want to be able to rewrite the review so that it is written more politely. We can design this as a prompt template which takes as inputs:\n",
    "- customer review : content of the review\n",
    "- style : what style to rewrite the review\n",
    "\n",
    "We wrap this in a prompt text, which takes the 'customer review' and 'style' as dynamic inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e38a8a13-45c7-4961-9608-917f452212ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_review = \"\"\"\n",
    "The food in this restaurant is honestly the \\\n",
    "worst I have ever had. The steak was so dry, \\\n",
    "it was like the cow went to the desert and died again, \\\n",
    "and the portion was so small. The staff \\\n",
    "were not helpful, took forever to come \\\n",
    "and didn't seem to care about providing \\\n",
    "a good customer experience. The meal was also \\\n",
    "grossly overpriced. Do not come here if you \\\n",
    "want good food.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3d89fe61-3591-4b5a-9264-c61a1b6a0c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "style = \"\"\"English in a polite tone.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1420902f-acbc-4fad-9533-ad9de3fc85b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate the text that is delimited by triple backticks \n",
      "into a style that is English in a polite tone..\n",
      "text: ```\n",
      "The food in this restaurant is honestly the worst I have ever had. The steak was so dry, it was like the cow went to the desert and died again, and the portion was so small. The staff were not helpful, took forever to come and didn't seem to care about providing a good customer experience. The meal was also grossly overpriced. Do not come here if you want good food.\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"Translate the text \\\n",
    "that is delimited by triple backticks \n",
    "into a style that is {style}.\n",
    "text: ```{customer_review}```\n",
    "\"\"\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "56266d7c-0999-419a-ae37-b92cae36aa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = get_completion(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8182bda3-090d-4ce3-bdeb-ba70187fd005",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I must express my disappointment with the food served at this restaurant. Unfortunately, the steak I ordered was extremely dry and the portion size was quite small. Additionally, the staff were not very attentive and seemed disinterested in providing a positive customer experience. Furthermore, the cost of the meal was quite high in comparison to the quality of the food. I would not recommend this restaurant to anyone seeking a satisfying dining experience.'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920ee6c6-25cf-4210-9fd1-a491bfd82ede",
   "metadata": {},
   "source": [
    "We now have a more dynamic template, where we can feed in some content (the review) and a style, and the response will vary accordingly. In the next section, we'll repeat this exercise but using LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41011011-4e49-43c6-bda4-0e1c4d4248b4",
   "metadata": {},
   "source": [
    "### Using LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b3f4a3-42a3-4b75-a754-4c5069a8d4f9",
   "metadata": {},
   "source": [
    "At the most basic level, we can think of there as being 3 components that make up a call to a LLM.\n",
    "We need:\n",
    "- A prompt (i.e. input) that will be fed into a LLM\n",
    "- A large language model that will read in the prompt as input and process it\n",
    "- A parser to take the output from the LLM and return it in a desired way\n",
    "\n",
    "Let's repeat the same exercise we did with OpenAI, but using LangChain this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aff657d4-7290-4515-969c-7bd37c9ec4f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install --upgrade langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e66a7a37-015d-48ff-8695-7dc7d6d5a0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I must express my disappointment with the quality of the food served at this establishment. Regrettably, the steak I ordered was excessively dry and the portion size was inadequate. Furthermore, the staff were unhelpful, took an unreasonable amount of time to attend to our needs, and appeared indifferent to providing a satisfactory customer experience. Additionally, the cost of the meal was exorbitant. I would advise against dining here if you are seeking a pleasurable culinary experience.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# define prompt template\n",
    "# input variables are denoted in {}\n",
    "template_string = \"\"\"Translate the text \\\n",
    "that is delimited by triple backticks \\\n",
    "into a style that is {style}. \\\n",
    "text: ```{text}```\n",
    "\"\"\"\n",
    "prompt_template = ChatPromptTemplate.from_template(template_string) # define prompt template\n",
    "chat = ChatOpenAI(temperature=0.0, model=llm_model) # define API endpoint / LLM\n",
    "\n",
    "# provide example prompt based off template design\n",
    "writing_style = \"\"\"English in a formal polite tone.\"\"\"\n",
    "customer_review = \"\"\"\n",
    "The food in this restaurant is honestly the \\\n",
    "worst I have ever had. The steak was so dry \\\n",
    "and the portion was so small. The staff \\\n",
    "were not helpful, took forever to come \\\n",
    "and didn't seem to care about providing \\\n",
    "a good customer experience. The meal was also \\\n",
    "grossly overpriced. Do not come here if you \\\n",
    "want good food.\n",
    "\"\"\"\n",
    "customer_messages = prompt_template.format_messages(\n",
    "                    style=writing_style,\n",
    "                    text=customer_review)\n",
    "\n",
    "# Call the LLM to translate to the style of the customer message\n",
    "customer_response = chat(customer_messages)\n",
    "\n",
    "# display response\n",
    "print(customer_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0af751-b9e8-49da-8e12-07fc6e945259",
   "metadata": {},
   "source": [
    "When using LangChain this time, we did not simply pass in a dynamic string, but we actually created a ChatPromptTemplate imported from the LangChain library. This gives us more flexibility, and makes for more modular and clean code. For example, we can actually see what the input variables to this template are as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5dd1a3e8-7d72-4702-96ad-8d269730de10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['style', 'text']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.messages[0].prompt.input_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a144ff66-0c12-4d94-bf1d-92d21177e364",
   "metadata": {},
   "source": [
    "The benefits of this will become more obvious once we start using more complex logic. Let's see another example now with an output parser."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9971ea59-00df-461c-b16a-125d5a46ee6e",
   "metadata": {},
   "source": [
    "### Parsing LLM output with LangChain\n",
    "\n",
    "We are going to look at another example, this time using a strucuted output parser. Suppose we have a listing description, and we want to extract specific information from this listing description, and output into a Pythin dictionary. This involves the following steps:\n",
    "- Design a prompt template which clearly takes a listing description, and specifices what information we want from it, and what format the output should be\n",
    "- Choose a LLM to process this prompt\n",
    "- Define an output parser, which will process the output from LLM and return in specific format\n",
    "\n",
    "Here, we will use as an example, the StructuredOutputParser.from_response_schemas() which will return a json from the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cd26b25d-4f16-4228-9176-40adf55dbcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2f2db916-f7e5-4a4a-8fc2-9bd3e02d907a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bedroom_schema = ResponseSchema(\n",
    "    name=\"bedroom\"\n",
    "    ,description=\"How many bedrooms does this property have? Answer as a single number if known. If unsure, Answer as Unknown.\")\n",
    "school_schema = ResponseSchema(\n",
    "    name=\"school\"\n",
    "    ,description=\"What schools are around the property? If this information is not found, output Unknown.\")\n",
    "amenity_schema = ResponseSchema(\n",
    "    name=\"amenity\"\n",
    "    ,description=\"Extract any amenties in the property, and output them as a comma separated Python list.\")\n",
    "\n",
    "response_schemas = [\n",
    "    bedroom_schema \n",
    "    ,school_schema\n",
    "    ,amenity_schema]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71a1025-9095-4f5d-a0fa-2acf6bc57d0e",
   "metadata": {},
   "source": [
    "We create multiple ResponseSchema, which reflects how we want to extract information using LLM, with a key. We put all of this into a response_schemas list, which will then be fed into a StructuredOutputParser.from_response_schemas() from LangChain to create and output_parser. We can then later use this output_parser to parse the outputs from LLM into a json.\n",
    "\n",
    "Additionally, the output_parser also helpfully has format instructions built in based on the provided response_schemas, which we can feed into the prompt later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "87c9cc50-1008-4d3a-aefa-0df148a74c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "789d6ec2-9e9b-496f-8d6d-6c065ac79f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"bedroom\": string  // How many bedrooms does this property have? Answer as a single number if known. If unsure, Answer as Unknown.\n",
      "\t\"school\": string  // What schools are around the property? If this information is not found, output Unknown.\n",
      "\t\"amenity\": string  // Extract any amenties in the property, and output them as a comma separated Python list.\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "format_instructions = output_parser.get_format_instructions()\n",
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48fbec5-0524-4aad-983f-5df6814f2dd2",
   "metadata": {},
   "source": [
    "We are now ready to define the prompt template. We see that there are two variables here:\n",
    "- text : This will be the listing description\n",
    "- format_instructions: This is simply how the output should be formatted - which we actually already have from the output parser above\n",
    "\n",
    "Note that the prompt itself still clearly states what information we want out of the listing description, and should match the keys in the format_instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c3724d67-756a-4023-860f-471ed23be36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define prompt template\n",
    "listing_info_format = \"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "bedroom: How many bedrooms does this property have? Answer as a single number if known. If unsure, Answer as Unknown.\n",
    "school: What schools are around the property? If this information is not found, output Unknown.\n",
    "amenity: Extract any amenties in the property, and output them as a comma separated Python list.\n",
    "\n",
    "text: {text}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template=listing_info_format)\n",
    "\n",
    "# give example input/prompt\n",
    "listing_description = \"\"\"\n",
    "Eva Building - Near New & Luxury Apartment with 2 Large Balconies\n",
    "Stylishly appointed this near-new three-bedroom apartment is perfectly located in the building of Eva Lane Cove. showcases a bright and versatile floor plan with spacious living and beautiful riverside views. Just footsteps to Hughes Park and a short stroll to city buses, cafes', shops and the bustling village also local schools.\n",
    "\n",
    "Features including:\n",
    "* Situated in sought-after location, enjoy parkside and riverside views\n",
    "* Generous 2 bedrooms plus a multi-function room\n",
    "* Large 2 balconies all with East aspects\n",
    "* Elegance 2-layer blackout curtains in the living area and bedrooms\n",
    "* Spacious interiors with a versatile open-plan living and dining area\n",
    "* Island modern kitchen with 'Millie' appliances, gas cooking and dishwasher\n",
    "* Three bedrooms all with built-in, the main bedroom with ensuite\n",
    "* Sparkling bathroom with floor-to-ceiling tiles\n",
    "* Ducting Air conditioning\n",
    "* Video intercom and internal laundry.\n",
    "* Secure one car space and storage\n",
    "\n",
    "Outgoings:\n",
    "Strata levy:$1208.60 pq\n",
    "Council rate: $359.00 pq\n",
    "Water: $158.45 pq approx.\n",
    "\"\"\"\n",
    "\n",
    "messages = prompt.format_messages(\n",
    "    text=listing_description, \n",
    "    format_instructions=format_instructions # give an output parser format defined previously\n",
    ")\n",
    "\n",
    "# feed input into model\n",
    "response = chat(messages)\n",
    "\n",
    "# parse model output into desired dictionary format\n",
    "output_dict = output_parser.parse(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "38ddd465-45d4-4117-9d04-d6e80062082a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bedroom': 3,\n",
       " 'school': 'Unknown',\n",
       " 'amenity': ['Island modern kitchen',\n",
       "  'Gas cooking',\n",
       "  'Dishwasher',\n",
       "  'Built-in wardrobes',\n",
       "  'Ensuite',\n",
       "  'Ducting Air conditioning',\n",
       "  'Video intercom',\n",
       "  'Internal laundry',\n",
       "  'Secure one car space and storage']}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc65266-a2e4-4546-aaeb-dc720a8fe5ca",
   "metadata": {},
   "source": [
    "Our final output is now in a dictionary as desired!\n",
    "\n",
    "To summarise, the key benefits from using LangChain so far is to have modularised code. It's clear which section is defining what we want to extract from the text, how we want the output formatted etc. If we didn't use LangChain, we would need to think really hard about how we structure our prompt to capture exactly what we want, a very daunting task!\n",
    "\n",
    "We've only just touched on the benefits of LangChain, lets keep exploring how else it can be helpful!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adf2c83-e135-4eed-a1a9-5353575d2f49",
   "metadata": {},
   "source": [
    "## Handling memory\n",
    "\n",
    "When we have a conversation with someone, our replies depend on what was talked about earlier. We keep a history of the context in our mind, which shape how we then respond. So far in our examples, there is no history or context provided when a response is generated. Here, we will look at an example where we have a conversation with AI, just like we would with a person."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a77ba05-8777-4ce8-b51a-102368b1b610",
   "metadata": {},
   "source": [
    "### ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09064f99-f07b-4d2f-a59a-60f46c0e1e34",
   "metadata": {},
   "source": [
    "We import the required libraries, and then initialise:\n",
    "- LLM as ChatOpenAPI with GPT\n",
    "- initialise memory object, ConversationBufferMemory\n",
    "- initialise a ConversationChain object from langchain, with specified LLM and memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "61fd52db-48da-45d9-b4f1-51b8bfa6a3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c0ab9734-8b3d-4433-84de-a4cc9b847508",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.0, model=llm_model)\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ae4c48-b568-44dd-9570-385f4f0a4fe3",
   "metadata": {},
   "source": [
    "Let's now start a conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0b5ad215-117f-4919-a6be-b6ada16209cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, my name is Jeffrey. My favourite sport is tennis.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello Jeffrey, it's nice to meet you! Tennis is a great sport. Did you know that it originated in 12th century France as a game played with the palm of the hand? It wasn't until the 16th century that rackets were introduced. Today, tennis is played all over the world and is a popular spectator sport as well. Do you have a favourite tennis player?\""
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi, my name is Jeffrey. My favourite sport is tennis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b752d257-a024-45fa-806a-5d9411080a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Jeffrey. My favourite sport is tennis.\n",
      "AI: Hello Jeffrey, it's nice to meet you! Tennis is a great sport. Did you know that it originated in 12th century France as a game played with the palm of the hand? It wasn't until the 16th century that rackets were introduced. Today, tennis is played all over the world and is a popular spectator sport as well. Do you have a favourite tennis player?\n",
      "Human: My favourite player is Roger Federer.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Ah, Roger Federer is a great choice! He has won a record 20 Grand Slam singles titles and has been ranked world No. 1 in men's singles tennis by the Association of Tennis Professionals a record total of 310 weeks. He is known for his elegant playing style and his ability to play well on all surfaces. Have you ever seen him play in person?\""
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"My favourite player is Roger Federer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "6fa61b70-5089-4264-b568-f534eff7f3b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Jeffrey. My favourite sport is tennis.\n",
      "AI: Hello Jeffrey, it's nice to meet you! Tennis is a great sport. Did you know that it originated in 12th century France as a game played with the palm of the hand? It wasn't until the 16th century that rackets were introduced. Today, tennis is played all over the world and is a popular spectator sport as well. Do you have a favourite tennis player?\n",
      "Human: My favourite player is Roger Federer.\n",
      "AI: Ah, Roger Federer is a great choice! He has won a record 20 Grand Slam singles titles and has been ranked world No. 1 in men's singles tennis by the Association of Tennis Professionals a record total of 310 weeks. He is known for his elegant playing style and his ability to play well on all surfaces. Have you ever seen him play in person?\n",
      "Human: What is my name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Your name is Jeffrey, as you mentioned earlier.'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718983b1-caf3-4781-8f9e-b587d9ebfa4e",
   "metadata": {},
   "source": [
    "We see that we can have a conversation with AI, then after multiple inputs, ask it again what is my name. The AI remembers my name even though it was from a conversation from awhile back. Using this conversation chain with the defined memory object, we can now store the historical conversation.\n",
    "\n",
    "We can also print out the conversation up till now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "06ce979c-e0c1-462b-a938-46b13ede31a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi, my name is Jeffrey. My favourite sport is tennis.\n",
      "AI: Hello Jeffrey, it's nice to meet you! Tennis is a great sport. Did you know that it originated in 12th century France as a game played with the palm of the hand? It wasn't until the 16th century that rackets were introduced. Today, tennis is played all over the world and is a popular spectator sport as well. Do you have a favourite tennis player?\n",
      "Human: My favourite player is Roger Federer.\n",
      "AI: Ah, Roger Federer is a great choice! He has won a record 20 Grand Slam singles titles and has been ranked world No. 1 in men's singles tennis by the Association of Tennis Professionals a record total of 310 weeks. He is known for his elegant playing style and his ability to play well on all surfaces. Have you ever seen him play in person?\n",
      "Human: What is my name?\n",
      "AI: Your name is Jeffrey, as you mentioned earlier.\n"
     ]
    }
   ],
   "source": [
    "# see the stored history\n",
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cdb3da-14c3-480c-8148-600201175c59",
   "metadata": {},
   "source": [
    "We can also explicitly set a conversation as context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "45c42a56-6c7b-437c-9d91-a55a46ecce9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi\n",
      "AI: What's up\n"
     ]
    }
   ],
   "source": [
    "memory = ConversationBufferMemory()\n",
    "memory.save_context(\n",
    "    {\"input\": \"Hi\"}, \n",
    "    {\"output\": \"What's up\"})\n",
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31618de-cae8-4099-8d6d-025f73321dc4",
   "metadata": {},
   "source": [
    "### How do LLM store memory?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33c2ea5-ce78-47b4-b33f-e7add757726b",
   "metadata": {},
   "source": [
    "Large Language Models are stateless, meaning that each call to the API is independent. The memory that we see is actually from us providing the full conversation up to that point as the context. We saw earlier that the memory buffer had the entire conversation history stored. However, overtime as the conversation gets longer, the memory will eventually be too big and costly to keep passing in as input. LangChain provides many different ways to handle its memory. We have so far seen the most basic, ConversationBufferMemory() which just stores all the conversation up to that point as context. Let's explore other memories in LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273ecd4e-a07b-4dec-a710-5eaa9210491a",
   "metadata": {},
   "source": [
    "### ConversationBufferWindowMemory\n",
    "\n",
    "One way to manage storage of history is to only keep the last 'k' conversations in history. We can do this with the ConversationBufferWindowMemory, which has a parameter 'k'.\n",
    "\n",
    "For example, below we set k=1. We can see from the loaded memory variables that it has only stored the most recent conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "3b0c68e3-282b-41d9-abc6-03e21b2495bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: Yes, he is one of my favourite players.\\nAI: He has over 20 grand slams.'}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k=1) \n",
    "memory.save_context({\"input\": \"Hi, my name is Jeffrey and I like tennis\"},\n",
    "                    {\"output\": \"Hi Jeffrey, have you heard of Roger Federer?\"})\n",
    "memory.save_context({\"input\": \"Yes, he is one of my favourite players.\"},\n",
    "                    {\"output\": \"He has over 20 grand slams.\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f80f808-c22e-4c45-925b-be25ba7040f0",
   "metadata": {},
   "source": [
    "We can demonstrate this in a conversation as well, where we can see below that the AI can not remember what my name is, even though it was introduced in the beginning. This is because the window is only k=1, so it only stores the most recent conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a9fd0c9f-7e45-4a34-926c-cfc70d749aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.0, model=llm_model)\n",
    "memory = ConversationBufferWindowMemory(k=1)\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = memory,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "7c77844a-d7de-4fce-b656-41941984ec85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello Jeffrey, it's nice to meet you. My name is OpenAI. How can I assist you today?\""
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi, my name is Jeffrey.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "ffc38ff8-2251-44c4-b909-dd3adabe5fa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1 + 1 equals 2, as it is a basic arithmetic operation.'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is 1 + 1.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "0f482dbf-51fd-48a7-ba42-ccd97384e971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, I don't have access to that information. Could you please tell me your name?\""
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is my name.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1da68c6-0793-41d4-bd34-3e8993d616e7",
   "metadata": {},
   "source": [
    "### ConversationTokenBufferMemory\n",
    "\n",
    "Another way to store memory is to store a certain number of tokens (e.g. characters) in the history, rather than number of conversations. Conversations can vary in length, and some can be very long, so storing by tokens is a more consistent way of storing history from a memory perspective. Costs for API calls are usually also dependent on token length, so this method is more directly related to the cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "8d826089-7921-4af3-bffa-025c316ffc67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3365f35-ae8a-45c8-a638-28b9ff6ce9b5",
   "metadata": {},
   "source": [
    "Let's start by setting the max token limit to 100. In this example, we see that it stores only the most recent conversation. This is because it is under 100 tokens, however if we were to include the conversation earlier it would be over 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "6c4e4e19-c43d-46f3-84b8-58869520f67d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: My favourite player is Roger Federer.\\nAI: Ah, Roger Federer is a great choice! He has won a record 20 Grand Slam singles titles and has been ranked world No. 1 in men's singles tennis by the Association of Tennis Professionals a record total of 310 weeks. He is known for his elegant playing style and his ability to play well on all surfaces. Have you ever seen him play in person?\"}"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "from langchain.llms import OpenAI\n",
    "llm = ChatOpenAI(temperature=0.0, model=llm_model)\n",
    "\n",
    "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=100)\n",
    "memory.save_context({\"input\": \"Hi, my name is Jeffrey. My favourite sport is tennis.\"},\n",
    "                    {\"output\": \"Hello Jeffrey, it's nice to meet you! Tennis is a great sport. Did you know that it originated in 12th century France as a game played with the palm of the hand? It wasn't until the 16th century that rackets were introduced. Today, tennis is played all over the world and is a popular spectator sport as well. Do you have a favourite tennis player?\"})\n",
    "memory.save_context({\"input\": \"My favourite player is Roger Federer.\"},\n",
    "                    {\"output\": \"Ah, Roger Federer is a great choice! He has won a record 20 Grand Slam singles titles and has been ranked world No. 1 in men's singles tennis by the Association of Tennis Professionals a record total of 310 weeks. He is known for his elegant playing style and his ability to play well on all surfaces. Have you ever seen him play in person?\"})\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91c3d61-8d50-4c79-9e35-d716b334672a",
   "metadata": {},
   "source": [
    "If instead we set the max token limit to 10, we see that it does not store anything, becase the most recent conversation has over 10 tokens. It will only store a conversation if it is under the max token limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "db65d88d-7154-4919-9076-4fafe781551c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': ''}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=10)\n",
    "memory.save_context({\"input\": \"Hi, my name is Jeffrey. My favourite sport is tennis.\"},\n",
    "                    {\"output\": \"Hello Jeffrey, it's nice to meet you! Tennis is a great sport. Did you know that it originated in 12th century France as a game played with the palm of the hand? It wasn't until the 16th century that rackets were introduced. Today, tennis is played all over the world and is a popular spectator sport as well. Do you have a favourite tennis player?\"})\n",
    "memory.save_context({\"input\": \"My favourite player is Roger Federer.\"},\n",
    "                    {\"output\": \"Ah, Roger Federer is a great choice! He has won a record 20 Grand Slam singles titles and has been ranked world No. 1 in men's singles tennis by the Association of Tennis Professionals a record total of 310 weeks. He is known for his elegant playing style and his ability to play well on all surfaces. Have you ever seen him play in person?\"})\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9819cb2f-b35b-4e90-8c86-63c26d168c19",
   "metadata": {},
   "source": [
    "### ConversationSummaryBufferMemory\n",
    "\n",
    "When conversations get really long, both the window and token methods are not satisfactory, since they will start dropping more and more of the conversations if costs are to be maintained. A neat trick to get around this is to use the ConversationSummaryBufferMemory. This object similarly has a max_token_limit like before, however when the memory is past this max_token_limit, it will summarise all the conversations up to now using LLM, and then keep the summarised conversations as the context instead. It will then process new conversations as usual until the max_token_limit is reach once again, and it will summarise the conversations thus far again, and so on.\n",
    "\n",
    "Let's see an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "8de5f8b7-23d0-4f03-b97a-4179d0b79dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "# create a long string\n",
    "listing_description = \"\"\"\n",
    "Eva Building - Near New & Luxury Apartment with 2 Large Balconies\n",
    "Stylishly appointed this near-new three-bedroom apartment is perfectly located in the building of Eva Lane Cove. showcases a bright and versatile floor plan with spacious living and beautiful riverside views. Just footsteps to Hughes Park and a short stroll to city buses, cafes', shops and the bustling village also local schools.\n",
    "\n",
    "Features including:\n",
    "* Situated in sought-after location, enjoy parkside and riverside views\n",
    "* Generous 2 bedrooms plus a multi-function room\n",
    "* Large 2 balconies all with East aspects\n",
    "* Elegance 2-layer blackout curtains in the living area and bedrooms\n",
    "* Spacious interiors with a versatile open-plan living and dining area\n",
    "* Island modern kitchen with 'Millie' appliances, gas cooking and dishwasher\n",
    "* Three bedrooms all with built-in, the main bedroom with ensuite\n",
    "* Sparkling bathroom with floor-to-ceiling tiles\n",
    "* Ducting Air conditioning\n",
    "* Video intercom and internal laundry.\n",
    "* Secure one car space and storage\n",
    "\n",
    "Outgoings:\n",
    "Strata levy:$1208.60 pq\n",
    "Council rate: $359.00 pq\n",
    "Water: $158.45 pq approx.\n",
    "\"\"\"\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=200)\n",
    "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})\n",
    "memory.save_context({\"input\": \"Give me a property listing,\"}, \n",
    "                    {\"output\": f\"{listing_description}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "480191cf-9137-4ac8-acb9-29558398efda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'System: The human greets the AI and asks for a property listing. The AI provides a detailed description of a luxury apartment with riverside views, spacious living areas, and modern amenities. The apartment is located near parks, cafes, shops, and schools. The outgoings include strata levy, council rate, and water fees.'}"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b03ff7-32f4-41ac-8bf6-e15bbb39d62f",
   "metadata": {},
   "source": [
    "In above example, we give the memory a long property listing. Since it is more than 200 tokens, it is summarised in a few short sentences and this then becomes its context/memory.\n",
    "\n",
    "If we then prompt this further, the AI will reply, having the summarised context. The new AI response is now also part of the memory - with the latest response not being summarised since in total it is still under 200 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a57c1f0c-89bc-4919-97ba-3c41e875ab65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: The human greets the AI and asks for a property listing. The AI provides a detailed description of a luxury apartment with riverside views, spacious living areas, and modern amenities. The apartment is located near parks, cafes, shops, and schools. The outgoings include strata levy, council rate, and water fees.\n",
      "Human: Do you think it is a good property?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Based on the features and location of the property, it appears to be a high-quality and desirable option for those seeking luxury living with convenient access to local amenities. However, whether or not it is a good property ultimately depends on individual preferences and needs.'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = memory,\n",
    "    verbose=True\n",
    ")\n",
    "conversation.predict(input=\"Do you think it is a good property?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "c1218fc1-5912-4e76-8b2a-97a1b2c46653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'System: The human greets the AI and asks for a property listing. The AI provides a detailed description of a luxury apartment with riverside views, spacious living areas, and modern amenities. The apartment is located near parks, cafes, shops, and schools. The outgoings include strata levy, council rate, and water fees.\\nHuman: Do you think it is a good property?\\nAI: Based on the features and location of the property, it appears to be a high-quality and desirable option for those seeking luxury living with convenient access to local amenities. However, whether or not it is a good property ultimately depends on individual preferences and needs.'}"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f7a72f-4d57-4c9b-aa48-25e4e9064fcc",
   "metadata": {},
   "source": [
    "### Other memory methods\n",
    "\n",
    "There are many other memory types, and you are encouraged to have a look at what other options are there. These include:\n",
    "- Vector data memory - stores text in a vector database and retrives the most relevant blocks of text\n",
    "- Entity memories - remembers details about specific entities\n",
    "- and more...\n",
    "\n",
    "We can use multiple memories at one time, so go experiment!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b61afeb-b5d2-48bc-86ad-0d2c7e223c03",
   "metadata": {},
   "source": [
    "## Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fd44bf-2d85-4196-8532-4286ab260c4a",
   "metadata": {},
   "source": [
    "### What is a chain?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e966d8-5fdc-42b1-a4cb-805d0646b969",
   "metadata": {},
   "source": [
    "### LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "9b662689-709b-4754-a8d2-d8e6518accbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Property Finder\" or \"Listing Compare\" could be good names for a product that compares property listings.'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.9, model=llm_model)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"What is the best name to describe \\\n",
    "    a product that {product}?\"\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "product = \"compares property listings\"\n",
    "chain.run(product)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6af2db7-468f-4f44-9779-0976864727d4",
   "metadata": {},
   "source": [
    "### SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "4014ba0c-e764-42a0-968a-27b27fc22fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "e7ed9d02-bdd8-4b19-b351-5105235d9e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.9, model=llm_model)\n",
    "\n",
    "# prompt template 1\n",
    "first_prompt = ChatPromptTemplate.from_template(\n",
    "    \"What is the best name to describe \\\n",
    "    a product that {product_function}?\"\n",
    ")\n",
    "\n",
    "# Chain 1\n",
    "chain_one = LLMChain(llm=llm, prompt=first_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "fefcafad-0a51-4841-99b9-3a5933e9000c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt template 2\n",
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Write a 20 words description for the following \\\n",
    "    product:{product_description}\"\n",
    ")\n",
    "# chain 2\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "59140eff-4564-47fd-93fb-ccf71ca24490",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_simple_chain = SimpleSequentialChain(\n",
    "    chains=[chain_one, chain_two],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "ae55e77a-53ce-4497-b3ae-3d420638d970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m\"Listings Comparison Tool\" or \"Property Comparison Engine\" are some good names to describe a product that compares property listings.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mA tool that enables property buyers to easily compare different listings, making the search process quicker and more efficient.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'A tool that enables property buyers to easily compare different listings, making the search process quicker and more efficient.'"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_function = \"compares property listings\"\n",
    "overall_simple_chain.run(product_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1e93da-4fdb-4de7-b8da-4f4d6a8dfaaf",
   "metadata": {},
   "source": [
    "### SequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "ce2c3b2e-3aef-4f6a-b822-f01e27524b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "f296e962-db46-4867-96fe-16fdce45f06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.9, model=llm_model)\n",
    "\n",
    "# prompt template 1\n",
    "first_prompt = ChatPromptTemplate.from_template(\n",
    "    \"What is the best name to describe \\\n",
    "    a product that {product_function}?\"\n",
    ")\n",
    "\n",
    "# Chain 1\n",
    "chain_one = LLMChain(llm=llm, prompt=first_prompt, output_key=\"product_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "8611c120-76f0-458e-85a8-9e82748e0721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt template 2\n",
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Write a 20 words description for the following \\\n",
    "    product:{product_name}\"\n",
    ")\n",
    "# chain 2\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt, output_key=\"product_description\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "a53bd5f4-6cd7-4ac9-b76b-ee4dea79c821",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_chain = SequentialChain(\n",
    "    chains=[chain_one, chain_two],\n",
    "    input_variables=[\"product_function\"],\n",
    "    output_variables=[\"product_name\",\"product_description\"],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "631729e6-7515-43d4-96cf-16359613f48c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'product_function': 'compares property listings',\n",
       " 'product_name': '\"Listings Comparator\" or \"Property Search Comparator\"',\n",
       " 'product_description': 'Our Listings/Property Search Comparator is a tool that helps you compare and find the best real estate listings based on your preferences.'}"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_function = \"compares property listings\"\n",
    "overall_chain(product_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171e2c0d-f605-4c30-abba-b6cc15476aae",
   "metadata": {},
   "source": [
    "### RouterChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6febc0-6fde-4eab-b862-9afb77bba825",
   "metadata": {},
   "source": [
    "### Other chains to explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0068c1de-e85d-4adc-82dd-0beffa9c1cdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
