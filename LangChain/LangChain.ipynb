{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e745f09-60fe-49a5-ab7e-1062530d6590",
   "metadata": {},
   "source": [
    "# LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405ae33b-921c-4099-ac99-5ed07c71f339",
   "metadata": {},
   "source": [
    "Large Language Models (LLM) have become increasingly used in more and more complex applications. When we use LLM in applications such as chatbots, there are many things that are happening that need to be considered:\n",
    "- How do we keep track of historical context?\n",
    "- Can we simulate a conversation with different people / agents?\n",
    "- Can we direct the conversation to different 'people' depending on what is asked? (e.g. asking a panel of experts, and picking the most relevant expert based on the question)\n",
    "\n",
    "We need a framework that allows us to abstract away many of these complexities, and ideally make it easy to logically structure how we use these LLMs. Introducing LangChain:\n",
    "\n",
    "LangChain is an open-source framework for developing applications powered by LLM. This framework includes the ability to:\n",
    "- Efficiently integrate with popular AI platforms such as OpenAI (company behind ChatGPT) and Hugging Face\n",
    "- Connecting language driven models to data sources\n",
    "- Enable LLMs to interact dynamically with their environment\n",
    "\n",
    "It is designed to have modular components, that when combined together, can be used in many different applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15965ef8-7c1c-4154-89e4-3e1f91027efa",
   "metadata": {},
   "source": [
    "## Course outline\n",
    "- Models, Prompts and Output Parsers\n",
    "    - Calling OpenAI\n",
    "    - OpenAI Endpoints\n",
    "    - Prompt templates\n",
    "    - Using LangChain\n",
    "- Handling memory\n",
    "    - How do LLM store memory?\n",
    "    - ConversationBufferMemory\n",
    "    - ConversationBufferWindowMemory\n",
    "    - ConversationTokenBufferMemory\n",
    "    - ConverastionSummaryBufferMemory\n",
    "    - Other memory methods\n",
    "- Chains\n",
    "    - What is a chain?\n",
    "    - LLMChain\n",
    "    - SimpleSequentialChain\n",
    "    - SequentialChain\n",
    "    - RouterChain\n",
    "    - Other chains to explore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414125ee-3b49-4701-8f48-63c3e8d713e9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Models, Prompts and Output Parsers\n",
    "\n",
    "### Calling OpenAI\n",
    "\n",
    "Before we look into LangChain and what it can do, we will make direct calls to OpenAI to show you what LLMs can do.\n",
    "\n",
    "Let's look at an example to see how this works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de870e72-b2e6-4c52-b725-d2e189c982e4",
   "metadata": {},
   "source": [
    "If want to follow along, you will need your own OpenAI API key.\n",
    "\n",
    "Follow this link to get your own API key:\n",
    "https://platform.openai.com/account/api-keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a0f1a8b-fbdb-478e-8de6-306e0524ef52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install python-dotenv\n",
    "# !pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "916e64a2-4630-4415-8d5f-e328d7757f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your API key here\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "639fa87c-21e5-4f3b-91be-c92ac9224afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66038294-e2a3-4288-a9d9-507fd449d744",
   "metadata": {},
   "source": [
    "### OpenAI Endpoints\n",
    "\n",
    "An API (application programming interface) is a software intermediary that allows applications to talk to each other. An API endpoint is a specific location within an API that accepts requests and sends responses back.\n",
    "\n",
    "OpenAI has several API endpoints that are offered. This includes tasks such as:\n",
    "- Audio files to text\n",
    "- Chat responses given list of messages\n",
    "- Predicted text completion\n",
    "- Create vector embeddings given a text input\n",
    "- Generate images given prompts and image\n",
    "- and many more...\n",
    "\n",
    "It is fascinating how simple it is now to access all this through a simple API call.\n",
    "\n",
    "We will demonstrate using openAI's ChatCompletion API. This is an API that is useful for having conversations, where given a list of messages comprising a conversation, it will return a response (think like a Chatbot). Let's start by keeping it simple, and just demonstrate what it looks like to call this API endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5dbc1d-c8d1-41cc-8a61-c341350b049d",
   "metadata": {},
   "source": [
    "Let us choose ChatGPT as the LLM for this demo. We account for the deprecation of the LLM by comparing to the target date of June 12th 2024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7a3ed6e1-0b51-4c61-9a25-a55d3f09ecf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# account for deprecation of LLM model\n",
    "import datetime\n",
    "\n",
    "current_date = datetime.datetime.now().date() # Get the current date\n",
    "target_date = datetime.date(2024, 6, 12) # Define the date after which the model should be set to \"gpt-3.5-turbo\"\n",
    "\n",
    "# Set the model variable based on the current date\n",
    "if current_date > target_date:\n",
    "    llm_model = \"gpt-3.5-turbo\"\n",
    "else:\n",
    "    llm_model = \"gpt-3.5-turbo-0301\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663434e7-2b59-4a8e-977f-31ca63144224",
   "metadata": {},
   "source": [
    "We will now define a function that will take in a prompt, feed it to our chosen LLM, then return the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "095c773d-1572-4b8a-8e16-8c484c010050",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=llm_model):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0, \n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ab3da822-d3d1-4411-a9b2-985f4e58e72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this converations, we have different roles (e.g. a customer, and an assistant), and also a 'system' role, which you can think of background information that defines who the assistant is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "590e8f91-d7a6-4ccb-aa8f-65e16fc05167",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of Australia is Canberra.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_completion(prompt=\"What is the capital of Australia?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba40591-6fcb-4a04-bdae-2009607c528f",
   "metadata": {},
   "source": [
    "Here, we simply called the ChatCompletion API endpoint with a simple prompt, and got a response. A further breakdown of the function:\n",
    "- messages : this is the list of messages we are going to pass to the ChatCompletion API endpoint\n",
    "- response : this is the API endpoint, that takes the messages as input, uses the GPT-3.5 model to process and return a response\n",
    "- temperature : this is a parameter that defines how random the response should be. 0 is telling the model to be more deterministic.\n",
    "\n",
    "Let's now explore what else we can do with this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0df964-ff5c-4975-9001-450ef823fb73",
   "metadata": {},
   "source": [
    "### Prompt templates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558bfe92-7c2a-44d9-8259-1be3f47aba26",
   "metadata": {},
   "source": [
    "Let's now customise the prompt/input, so that it is more dynamic, and like a template we can reuse.\n",
    "\n",
    "Suppose we have a customer review of a restaurant they went to. It is written in a very rude tone. We want to be able to rewrite the review so that it is written more politely. We can design this as a prompt template which takes as inputs:\n",
    "- customer review : content of the review\n",
    "- style : what style to rewrite the review\n",
    "\n",
    "We wrap this in a prompt text, which takes the 'customer review' and 'style' as dynamic inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e38a8a13-45c7-4961-9608-917f452212ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_review = \"\"\"\n",
    "The food in this restaurant is honestly the \\\n",
    "worst I have ever had. The steak was so dry \\\n",
    "and the portion was so small. The staff \\\n",
    "were not helpful, took forever to come \\\n",
    "and didn't seem to care about providing \\\n",
    "a good customer experience. The meal was also \\\n",
    "grossly overpriced. Do not come here if you \\\n",
    "want good food.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3d89fe61-3591-4b5a-9264-c61a1b6a0c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "style = \"\"\"English in a polite tone.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1420902f-acbc-4fad-9533-ad9de3fc85b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate the text that is delimited by triple backticks \n",
      "into a style that is English in a polite tone..\n",
      "text: ```\n",
      "The food in this restaurant is honestly the worst I have ever had. The steak was so dry and the portion was so small. The staff were not helpful, took forever to come and didn't seem to care about providing a good customer experience. The meal was also grossly overpriced. Do not come here if you want good food.\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"Translate the text \\\n",
    "that is delimited by triple backticks \n",
    "into a style that is {style}.\n",
    "text: ```{customer_review}```\n",
    "\"\"\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "56266d7c-0999-419a-ae37-b92cae36aa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = get_completion(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8182bda3-090d-4ce3-bdeb-ba70187fd005",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I must say that I was disappointed with the food at this restaurant. Unfortunately, the steak I ordered was quite dry and the portion size was rather small. Additionally, the staff were not very helpful and took quite a long time to attend to our needs. It seemed as though they were not very concerned with providing a positive customer experience. Furthermore, the meal was quite expensive and did not seem to be worth the price. I would not recommend this restaurant if you are looking for good food.'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920ee6c6-25cf-4210-9fd1-a491bfd82ede",
   "metadata": {},
   "source": [
    "We now have a more dynamic template, where we can feed in some content (the review) and a style, and the response will vary accordingly. In the next section, we'll repeat this exercise but using LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41011011-4e49-43c6-bda4-0e1c4d4248b4",
   "metadata": {},
   "source": [
    "### Using LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b3f4a3-42a3-4b75-a754-4c5069a8d4f9",
   "metadata": {},
   "source": [
    "At the most basic level, we can think of there as being 3 components that make up a call to a LLM.\n",
    "We need:\n",
    "- A prompt (i.e. input) that will be fed into a LLM\n",
    "- A large language model that will read in the prompt as input and process it\n",
    "- A parser to take the output from the LLM and return it in a desired way\n",
    "\n",
    "Let's repeat the same exercise we did with OpenAI, but using LangChain this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aff657d4-7290-4515-969c-7bd37c9ec4f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install --upgrade langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e66a7a37-015d-48ff-8695-7dc7d6d5a0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I must express my disappointment with the quality of the food served at this establishment. Regrettably, the steak I ordered was excessively dry and the portion size was inadequate. Furthermore, the staff were unhelpful, took an unreasonable amount of time to attend to our needs, and appeared indifferent to providing a satisfactory customer experience. Additionally, the cost of the meal was exorbitant. I would advise against dining here if you are seeking a pleasurable culinary experience.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# define prompt template\n",
    "# input variables are denoted in {}\n",
    "template_string = \"\"\"Translate the text \\\n",
    "that is delimited by triple backticks \\\n",
    "into a style that is {style}. \\\n",
    "text: ```{text}```\n",
    "\"\"\"\n",
    "prompt_template = ChatPromptTemplate.from_template(template_string) # define prompt template\n",
    "chat = ChatOpenAI(temperature=0.0, model=llm_model) # define API endpoint / LLM\n",
    "\n",
    "# provide example prompt based off template design\n",
    "writing_style = \"\"\"English in a formal polite tone.\"\"\"\n",
    "customer_review = \"\"\"\n",
    "The food in this restaurant is honestly the \\\n",
    "worst I have ever had. The steak was so dry \\\n",
    "and the portion was so small. The staff \\\n",
    "were not helpful, took forever to come \\\n",
    "and didn't seem to care about providing \\\n",
    "a good customer experience. The meal was also \\\n",
    "grossly overpriced. Do not come here if you \\\n",
    "want good food.\n",
    "\"\"\"\n",
    "customer_messages = prompt_template.format_messages(\n",
    "                    style=writing_style,\n",
    "                    text=customer_review)\n",
    "\n",
    "# Call the LLM to translate to the style of the customer message\n",
    "customer_response = chat(customer_messages)\n",
    "\n",
    "# display response\n",
    "print(customer_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0af751-b9e8-49da-8e12-07fc6e945259",
   "metadata": {},
   "source": [
    "When using LangChain this time, we did not simply pass in a dynamic string, but we actually created a ChatPromptTemplate imported from the LangChain library. This gives us more flexibility, and makes for more modular and clean code. For example, we can actually see what the input variables to this template are as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5dd1a3e8-7d72-4702-96ad-8d269730de10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['style', 'text']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.messages[0].prompt.input_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a144ff66-0c12-4d94-bf1d-92d21177e364",
   "metadata": {},
   "source": [
    "The benefits of this will become more obvious once we start using more complex logic. Let's see another example now with an output parser."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9971ea59-00df-461c-b16a-125d5a46ee6e",
   "metadata": {},
   "source": [
    "### Parsing LLM output with LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cd26b25d-4f16-4228-9176-40adf55dbcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2f2db916-f7e5-4a4a-8fc2-9bd3e02d907a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bedroom_schema = ResponseSchema(\n",
    "    name=\"bedroom\"\n",
    "    ,description=\"How many bedrooms does this property have? Answer as a single number if known. If unsure, Answer as Unknown.\")\n",
    "school_schema = ResponseSchema(\n",
    "    name=\"school\"\n",
    "    ,description=\"What schools are around the property? If this information is not found, output Unknown.\")\n",
    "amenity_schema = ResponseSchema(\n",
    "    name=\"amenity\"\n",
    "    ,description=\"Extract any amenties in the property, and output them as a comma separated Python list.\")\n",
    "\n",
    "response_schemas = [\n",
    "    bedroom_schema \n",
    "    ,school_schema\n",
    "    ,amenity_schema]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "87c9cc50-1008-4d3a-aefa-0df148a74c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "789d6ec2-9e9b-496f-8d6d-6c065ac79f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"bedroom\": string  // How many bedrooms does this property have? Answer as a single number if known. If unsure, Answer as Unknown.\n",
      "\t\"school\": string  // What schools are around the property? If this information is not found, output Unknown.\n",
      "\t\"amenity\": string  // Extract any amenties in the property, and output them as a comma separated Python list.\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "format_instructions = output_parser.get_format_instructions()\n",
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c3724d67-756a-4023-860f-471ed23be36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define prompt template\n",
    "listing_info_format = \"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "bedroom: How many bedrooms does this property have? Answer as a single number if known. If unsure, Answer as Unknown.\n",
    "school: What schools are around the property? If this information is not found, output Unknown.\n",
    "amenity: Extract any amenties in the property, and output them as a comma separated Python list.\n",
    "\n",
    "text: {text}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template=listing_info_format)\n",
    "\n",
    "# give example input/prompt\n",
    "listing_description = \"\"\"\n",
    "Eva Building - Near New & Luxury Apartment with 2 Large Balconies\n",
    "Stylishly appointed this near-new three-bedroom apartment is perfectly located in the building of Eva Lane Cove. showcases a bright and versatile floor plan with spacious living and beautiful riverside views. Just footsteps to Hughes Park and a short stroll to city buses, cafes', shops and the bustling village also local schools.\n",
    "\n",
    "Features including:\n",
    "* Situated in sought-after location, enjoy parkside and riverside views\n",
    "* Generous 2 bedrooms plus a multi-function room\n",
    "* Large 2 balconies all with East aspects\n",
    "* Elegance 2-layer blackout curtains in the living area and bedrooms\n",
    "* Spacious interiors with a versatile open-plan living and dining area\n",
    "* Island modern kitchen with 'Millie' appliances, gas cooking and dishwasher\n",
    "* Three bedrooms all with built-in, the main bedroom with ensuite\n",
    "* Sparkling bathroom with floor-to-ceiling tiles\n",
    "* Ducting Air conditioning\n",
    "* Video intercom and internal laundry.\n",
    "* Secure one car space and storage\n",
    "\n",
    "Outgoings:\n",
    "Strata levy:$1208.60 pq\n",
    "Council rate: $359.00 pq\n",
    "Water: $158.45 pq approx.\n",
    "\"\"\"\n",
    "\n",
    "messages = prompt.format_messages(\n",
    "    text=listing_description, \n",
    "    format_instructions=format_instructions # give an output parser format defined previously\n",
    ")\n",
    "\n",
    "# feed input into model\n",
    "response = chat(messages)\n",
    "\n",
    "# parse model output into desired dictionary format\n",
    "output_dict = output_parser.parse(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "38ddd465-45d4-4117-9d04-d6e80062082a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bedroom': 3,\n",
       " 'school': 'Unknown',\n",
       " 'amenity': ['Island modern kitchen',\n",
       "  'Gas cooking',\n",
       "  'Dishwasher',\n",
       "  'Built-in wardrobes',\n",
       "  'Ensuite',\n",
       "  'Ducting Air conditioning',\n",
       "  'Video intercom',\n",
       "  'Internal laundry',\n",
       "  'Secure one car space and storage']}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adf2c83-e135-4eed-a1a9-5353575d2f49",
   "metadata": {},
   "source": [
    "## Handling memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50a7158-5913-4f36-bab0-e48d78c67275",
   "metadata": {},
   "source": [
    "### How do LLM store memory?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a77ba05-8777-4ce8-b51a-102368b1b610",
   "metadata": {},
   "source": [
    "### ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273ecd4e-a07b-4dec-a710-5eaa9210491a",
   "metadata": {},
   "source": [
    "### ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1da68c6-0793-41d4-bd34-3e8993d616e7",
   "metadata": {},
   "source": [
    "### ConversationTokenBufferMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9819cb2f-b35b-4e90-8c86-63c26d168c19",
   "metadata": {},
   "source": [
    "### ConversationSummaryBufferMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f7a72f-4d57-4c9b-aa48-25e4e9064fcc",
   "metadata": {},
   "source": [
    "### Other memory methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b2ea49-8e21-4aae-800f-44d7868a5ee3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a89f69d-abad-4e7a-be70-3b24ff1f17e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
