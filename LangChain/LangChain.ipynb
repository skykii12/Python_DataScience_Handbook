{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e745f09-60fe-49a5-ab7e-1062530d6590",
   "metadata": {},
   "source": [
    "# LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405ae33b-921c-4099-ac99-5ed07c71f339",
   "metadata": {},
   "source": [
    "Large Language Models (LLM) have become increasingly used in more and more complex applications. When we use LLM in applications such as chatbots, there are many things that are happening that need to be considered:\n",
    "- How do we keep track of historical context?\n",
    "- Can we simulate a conversation with different people / agents?\n",
    "- Can we direct the conversation to different 'people' depending on what is asked? (e.g. asking a panel of experts, and picking the most relevant expert based on the question)\n",
    "\n",
    "We need a framework that allows us to abstract away many of these complexities, and ideally make it easy to logically structure how we use these LLMs. Introducing LangChain:\n",
    "\n",
    "LangChain is an open-source framework for developing applications powered by LLM. This framework includes the ability to:\n",
    "- Efficiently integrate with popular AI platforms such as OpenAI (company behind ChatGPT) and Hugging Face\n",
    "- Connecting language driven models to data sources\n",
    "- Enable LLMs to interact dynamically with their environment\n",
    "\n",
    "It is designed to have modular components, that when combined together, can be used in many different applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15965ef8-7c1c-4154-89e4-3e1f91027efa",
   "metadata": {},
   "source": [
    "## Course outline\n",
    "- Models, Prompts and Output Parsers\n",
    "    - Calling OpenAI\n",
    "    - OpenAI Endpoints\n",
    "    - Prompt templates\n",
    "    - Using LangChain\n",
    "- Handling memory\n",
    "    - ConversationBufferMemory\n",
    "    - How do LLM store memory?\n",
    "    - ConversationBufferWindowMemory\n",
    "    - ConversationTokenBufferMemory\n",
    "    - ConverastionSummaryBufferMemory\n",
    "    - Other memory methods\n",
    "- Chains\n",
    "    - What is a chain?\n",
    "    - LLMChain\n",
    "    - SimpleSequentialChain\n",
    "    - SequentialChain\n",
    "    - RouterChain\n",
    "    - Other chains to explore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414125ee-3b49-4701-8f48-63c3e8d713e9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Models, Prompts and Output Parsers\n",
    "\n",
    "### Calling OpenAI\n",
    "\n",
    "Before we look into LangChain and what it can do, we will make direct calls to OpenAI to show you what LLMs can do.\n",
    "\n",
    "Let's look at an example to see how this works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de870e72-b2e6-4c52-b725-d2e189c982e4",
   "metadata": {},
   "source": [
    "If want to follow along, you will need your own OpenAI API key.\n",
    "\n",
    "Follow this link to get your own API key:\n",
    "https://platform.openai.com/account/api-keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a0f1a8b-fbdb-478e-8de6-306e0524ef52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install python-dotenv\n",
    "# !pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "916e64a2-4630-4415-8d5f-e328d7757f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your API key here\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "639fa87c-21e5-4f3b-91be-c92ac9224afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66038294-e2a3-4288-a9d9-507fd449d744",
   "metadata": {},
   "source": [
    "### OpenAI Endpoints\n",
    "\n",
    "An API (application programming interface) is a software intermediary that allows applications to talk to each other. An API endpoint is a specific location within an API that accepts requests and sends responses back.\n",
    "\n",
    "OpenAI has several API endpoints that are offered. This includes tasks such as:\n",
    "- Audio files to text\n",
    "- Chat responses given list of messages\n",
    "- Predicted text completion\n",
    "- Create vector embeddings given a text input\n",
    "- Generate images given prompts and image\n",
    "- and many more...\n",
    "\n",
    "It is fascinating how simple it is now to access all this through a simple API call.\n",
    "\n",
    "We will demonstrate using openAI's ChatCompletion API. This is an API that is useful for having conversations, where given a list of messages comprising a conversation, it will return a response (think like a Chatbot). Let's start by keeping it simple, and just demonstrate what it looks like to call this API endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5dbc1d-c8d1-41cc-8a61-c341350b049d",
   "metadata": {},
   "source": [
    "Let us choose ChatGPT as the LLM for this demo. We account for the deprecation of the LLM by comparing to the target date of June 12th 2024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7a3ed6e1-0b51-4c61-9a25-a55d3f09ecf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# account for deprecation of LLM model\n",
    "import datetime\n",
    "\n",
    "current_date = datetime.datetime.now().date() # Get the current date\n",
    "target_date = datetime.date(2024, 6, 12) # Define the date after which the model should be set to \"gpt-3.5-turbo\"\n",
    "\n",
    "# Set the model variable based on the current date\n",
    "if current_date > target_date:\n",
    "    llm_model = \"gpt-3.5-turbo\"\n",
    "else:\n",
    "    llm_model = \"gpt-3.5-turbo-0301\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663434e7-2b59-4a8e-977f-31ca63144224",
   "metadata": {},
   "source": [
    "We will now define a function that will take in a prompt, feed it to our chosen LLM, then return the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "095c773d-1572-4b8a-8e16-8c484c010050",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=llm_model):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0, \n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ab3da822-d3d1-4411-a9b2-985f4e58e72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this converations, we have different roles (e.g. a customer, and an assistant), and also a 'system' role, which you can think of background information that defines who the assistant is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "590e8f91-d7a6-4ccb-aa8f-65e16fc05167",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of Australia is Canberra.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_completion(prompt=\"What is the capital of Australia?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba40591-6fcb-4a04-bdae-2009607c528f",
   "metadata": {},
   "source": [
    "Here, we simply called the ChatCompletion API endpoint with a simple prompt, and got a response. A further breakdown of the function:\n",
    "- messages : this is the list of messages we are going to pass to the ChatCompletion API endpoint\n",
    "- response : this is the API endpoint, that takes the messages as input, uses the GPT-3.5 model to process and return a response\n",
    "- temperature : this is a parameter that defines how random the response should be. 0 is telling the model to be more deterministic.\n",
    "\n",
    "Let's now explore what else we can do with this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0df964-ff5c-4975-9001-450ef823fb73",
   "metadata": {},
   "source": [
    "### Prompt templates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558bfe92-7c2a-44d9-8259-1be3f47aba26",
   "metadata": {},
   "source": [
    "Let's now customise the prompt/input, so that it is more dynamic, and like a template we can reuse.\n",
    "\n",
    "Suppose we have a customer review of a restaurant they went to. It is written in a very rude tone. We want to be able to rewrite the review so that it is written more politely. We can design this as a prompt template which takes as inputs:\n",
    "- customer review : content of the review\n",
    "- style : what style to rewrite the review\n",
    "\n",
    "We wrap this in a prompt text, which takes the 'customer review' and 'style' as dynamic inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e38a8a13-45c7-4961-9608-917f452212ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_review = \"\"\"\n",
    "The food in this restaurant is honestly the \\\n",
    "worst I have ever had. The steak was so dry, \\\n",
    "it was like the cow went to the desert and died again, \\\n",
    "and the portion was so small. The staff \\\n",
    "were not helpful, took forever to come \\\n",
    "and didn't seem to care about providing \\\n",
    "a good customer experience. The meal was also \\\n",
    "grossly overpriced. Do not come here if you \\\n",
    "want good food.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3d89fe61-3591-4b5a-9264-c61a1b6a0c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "style = \"\"\"English in a polite tone.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1420902f-acbc-4fad-9533-ad9de3fc85b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate the text that is delimited by triple backticks \n",
      "into a style that is English in a polite tone..\n",
      "text: ```\n",
      "The food in this restaurant is honestly the worst I have ever had. The steak was so dry, it was like the cow went to the desert and died again, and the portion was so small. The staff were not helpful, took forever to come and didn't seem to care about providing a good customer experience. The meal was also grossly overpriced. Do not come here if you want good food.\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"Translate the text \\\n",
    "that is delimited by triple backticks \n",
    "into a style that is {style}.\n",
    "text: ```{customer_review}```\n",
    "\"\"\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "56266d7c-0999-419a-ae37-b92cae36aa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = get_completion(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8182bda3-090d-4ce3-bdeb-ba70187fd005",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I must express my disappointment with the food served at this restaurant. Unfortunately, the steak I ordered was extremely dry and the portion size was quite small. Additionally, the staff were not very attentive and seemed disinterested in providing a positive customer experience. Furthermore, the cost of the meal was quite high in comparison to the quality of the food. I would not recommend this restaurant to anyone seeking a satisfying dining experience.'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920ee6c6-25cf-4210-9fd1-a491bfd82ede",
   "metadata": {},
   "source": [
    "We now have a more dynamic template, where we can feed in some content (the review) and a style, and the response will vary accordingly. In the next section, we'll repeat this exercise but using LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41011011-4e49-43c6-bda4-0e1c4d4248b4",
   "metadata": {},
   "source": [
    "### Using LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b3f4a3-42a3-4b75-a754-4c5069a8d4f9",
   "metadata": {},
   "source": [
    "At the most basic level, we can think of there as being 3 components that make up a call to a LLM.\n",
    "We need:\n",
    "- A prompt (i.e. input) that will be fed into a LLM\n",
    "- A large language model that will read in the prompt as input and process it\n",
    "- A parser to take the output from the LLM and return it in a desired way\n",
    "\n",
    "Let's repeat the same exercise we did with OpenAI, but using LangChain this time. In this exercise, we are importing a specific chat model, **ChatOpenAI** that LangChain supports. This is just one of the many use cases that LangChain supports, and ChatOpenAI is specifically useful for building chatbots. We will later see that LangChain is a much more general framework, not limited to chatbots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aff657d4-7290-4515-969c-7bd37c9ec4f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install --upgrade langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e66a7a37-015d-48ff-8695-7dc7d6d5a0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I must express my disappointment with the quality of the food served at this establishment. Regrettably, the steak I ordered was excessively dry and the portion size was inadequate. Furthermore, the staff were unhelpful, took an unreasonable amount of time to attend to our needs, and appeared indifferent to providing a satisfactory customer experience. Additionally, the cost of the meal was exorbitant. I would advise against dining here if you are seeking a pleasurable culinary experience.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# define prompt template\n",
    "# input variables are denoted in {}\n",
    "template_string = \"\"\"Translate the text \\\n",
    "that is delimited by triple backticks \\\n",
    "into a style that is {style}. \\\n",
    "text: ```{text}```\n",
    "\"\"\"\n",
    "prompt_template = ChatPromptTemplate.from_template(template_string) # define prompt template\n",
    "chat = ChatOpenAI(temperature=0.0, model=llm_model) # define API endpoint / LLM\n",
    "\n",
    "# provide example prompt based off template design\n",
    "writing_style = \"\"\"English in a formal polite tone.\"\"\"\n",
    "customer_review = \"\"\"\n",
    "The food in this restaurant is honestly the \\\n",
    "worst I have ever had. The steak was so dry \\\n",
    "and the portion was so small. The staff \\\n",
    "were not helpful, took forever to come \\\n",
    "and didn't seem to care about providing \\\n",
    "a good customer experience. The meal was also \\\n",
    "grossly overpriced. Do not come here if you \\\n",
    "want good food.\n",
    "\"\"\"\n",
    "customer_messages = prompt_template.format_messages(\n",
    "                    style=writing_style,\n",
    "                    text=customer_review)\n",
    "\n",
    "# Call the LLM to translate to the style of the customer message\n",
    "customer_response = chat(customer_messages)\n",
    "\n",
    "# display response\n",
    "print(customer_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0af751-b9e8-49da-8e12-07fc6e945259",
   "metadata": {},
   "source": [
    "When using LangChain this time, we did not simply pass in a dynamic string, but we actually created a ChatPromptTemplate imported from the LangChain library. This gives us more flexibility, and makes for more modular and clean code. For example, we can actually see what the input variables to this template are as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5dd1a3e8-7d72-4702-96ad-8d269730de10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['style', 'text']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.messages[0].prompt.input_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a144ff66-0c12-4d94-bf1d-92d21177e364",
   "metadata": {},
   "source": [
    "The benefits of this will become more obvious once we start using more complex logic. Let's see another example now with an output parser."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9971ea59-00df-461c-b16a-125d5a46ee6e",
   "metadata": {},
   "source": [
    "### Parsing LLM output with LangChain\n",
    "\n",
    "We are going to look at another example, this time using a strucuted output parser. Suppose we have a listing description, and we want to extract specific information from this listing description, and output into a Pythin dictionary. This involves the following steps:\n",
    "- Design a prompt template which clearly takes a listing description, and specifices what information we want from it, and what format the output should be\n",
    "- Choose a LLM to process this prompt\n",
    "- Define an output parser, which will process the output from LLM and return in specific format\n",
    "\n",
    "Here, we will use as an example, the StructuredOutputParser.from_response_schemas() which will return a json from the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cd26b25d-4f16-4228-9176-40adf55dbcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2f2db916-f7e5-4a4a-8fc2-9bd3e02d907a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bedroom_schema = ResponseSchema(\n",
    "    name=\"bedroom\"\n",
    "    ,description=\"How many bedrooms does this property have? Answer as a single number if known. If unsure, Answer as Unknown.\")\n",
    "school_schema = ResponseSchema(\n",
    "    name=\"school\"\n",
    "    ,description=\"What schools are around the property? If this information is not found, output Unknown.\")\n",
    "amenity_schema = ResponseSchema(\n",
    "    name=\"amenity\"\n",
    "    ,description=\"Extract any amenties in the property, and output them as a comma separated Python list.\")\n",
    "\n",
    "response_schemas = [\n",
    "    bedroom_schema \n",
    "    ,school_schema\n",
    "    ,amenity_schema]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71a1025-9095-4f5d-a0fa-2acf6bc57d0e",
   "metadata": {},
   "source": [
    "We create multiple ResponseSchema, which reflects how we want to extract information using LLM, with a key. We put all of this into a response_schemas list, which will then be fed into a StructuredOutputParser.from_response_schemas() from LangChain to create and output_parser. We can then later use this output_parser to parse the outputs from LLM into a json.\n",
    "\n",
    "Additionally, the output_parser also helpfully has format instructions built in based on the provided response_schemas, which we can feed into the prompt later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "87c9cc50-1008-4d3a-aefa-0df148a74c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "789d6ec2-9e9b-496f-8d6d-6c065ac79f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"bedroom\": string  // How many bedrooms does this property have? Answer as a single number if known. If unsure, Answer as Unknown.\n",
      "\t\"school\": string  // What schools are around the property? If this information is not found, output Unknown.\n",
      "\t\"amenity\": string  // Extract any amenties in the property, and output them as a comma separated Python list.\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "format_instructions = output_parser.get_format_instructions()\n",
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48fbec5-0524-4aad-983f-5df6814f2dd2",
   "metadata": {},
   "source": [
    "We are now ready to define the prompt template. We see that there are two variables here:\n",
    "- text : This will be the listing description\n",
    "- format_instructions: This is simply how the output should be formatted - which we actually already have from the output parser above\n",
    "\n",
    "Note that the prompt itself still clearly states what information we want out of the listing description, and should match the keys in the format_instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c3724d67-756a-4023-860f-471ed23be36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define prompt template\n",
    "listing_info_format = \"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "bedroom: How many bedrooms does this property have? Answer as a single number if known. If unsure, Answer as Unknown.\n",
    "school: What schools are around the property? If this information is not found, output Unknown.\n",
    "amenity: Extract any amenties in the property, and output them as a comma separated Python list.\n",
    "\n",
    "text: {text}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template=listing_info_format)\n",
    "\n",
    "# give example input/prompt\n",
    "listing_description = \"\"\"\n",
    "Eva Building - Near New & Luxury Apartment with 2 Large Balconies\n",
    "Stylishly appointed this near-new three-bedroom apartment is perfectly located in the building of Eva Lane Cove. showcases a bright and versatile floor plan with spacious living and beautiful riverside views. Just footsteps to Hughes Park and a short stroll to city buses, cafes', shops and the bustling village also local schools.\n",
    "\n",
    "Features including:\n",
    "* Situated in sought-after location, enjoy parkside and riverside views\n",
    "* Generous 2 bedrooms plus a multi-function room\n",
    "* Large 2 balconies all with East aspects\n",
    "* Elegance 2-layer blackout curtains in the living area and bedrooms\n",
    "* Spacious interiors with a versatile open-plan living and dining area\n",
    "* Island modern kitchen with 'Millie' appliances, gas cooking and dishwasher\n",
    "* Three bedrooms all with built-in, the main bedroom with ensuite\n",
    "* Sparkling bathroom with floor-to-ceiling tiles\n",
    "* Ducting Air conditioning\n",
    "* Video intercom and internal laundry.\n",
    "* Secure one car space and storage\n",
    "\n",
    "Outgoings:\n",
    "Strata levy:$1208.60 pq\n",
    "Council rate: $359.00 pq\n",
    "Water: $158.45 pq approx.\n",
    "\"\"\"\n",
    "\n",
    "messages = prompt.format_messages(\n",
    "    text=listing_description, \n",
    "    format_instructions=format_instructions # give an output parser format defined previously\n",
    ")\n",
    "\n",
    "# feed input into model\n",
    "response = chat(messages)\n",
    "\n",
    "# parse model output into desired dictionary format\n",
    "output_dict = output_parser.parse(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "38ddd465-45d4-4117-9d04-d6e80062082a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bedroom': 3,\n",
       " 'school': 'Unknown',\n",
       " 'amenity': ['Island modern kitchen',\n",
       "  'Gas cooking',\n",
       "  'Dishwasher',\n",
       "  'Built-in wardrobes',\n",
       "  'Ensuite',\n",
       "  'Ducting Air conditioning',\n",
       "  'Video intercom',\n",
       "  'Internal laundry',\n",
       "  'Secure one car space and storage']}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc65266-a2e4-4546-aaeb-dc720a8fe5ca",
   "metadata": {},
   "source": [
    "Our final output is now in a dictionary as desired!\n",
    "\n",
    "To summarise, the key benefits from using LangChain so far is to have modularised code. It's clear which section is defining what we want to extract from the text, how we want the output formatted etc. If we didn't use LangChain, we would need to think really hard about how we structure our prompt to capture exactly what we want, a very daunting task!\n",
    "\n",
    "We've only just touched on the benefits of LangChain, lets keep exploring how else it can be helpful!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adf2c83-e135-4eed-a1a9-5353575d2f49",
   "metadata": {},
   "source": [
    "## Handling memory\n",
    "\n",
    "When we have a conversation with someone, our replies depend on what was talked about earlier. We keep a history of the context in our mind, which shape how we then respond. So far in our examples, there is no history or context provided when a response is generated. Here, we will look at an example where we have a conversation with AI, just like we would with a person."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a77ba05-8777-4ce8-b51a-102368b1b610",
   "metadata": {},
   "source": [
    "### ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09064f99-f07b-4d2f-a59a-60f46c0e1e34",
   "metadata": {},
   "source": [
    "We import the required libraries, and then initialise:\n",
    "- A model, specifically a ChatOpenAPI LLM\n",
    "- initialise memory object, ConversationBufferMemory\n",
    "- initialise a ConversationChain chain from langchain, with specified LLM and memory (we will learn more later about what a Chain is. For now think of it was a wrapper or building block that can take inputs, pass them into a model, and give an output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "61fd52db-48da-45d9-b4f1-51b8bfa6a3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c0ab9734-8b3d-4433-84de-a4cc9b847508",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.0, model=llm_model)\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ae4c48-b568-44dd-9570-385f4f0a4fe3",
   "metadata": {},
   "source": [
    "Let's now start a conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0b5ad215-117f-4919-a6be-b6ada16209cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, my name is Jeffrey. My favourite sport is tennis.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello Jeffrey, it's nice to meet you! Tennis is a great sport. Did you know that it originated in 12th century France as a game played with the palm of the hand? It wasn't until the 16th century that rackets were introduced. Today, tennis is played all over the world and is a popular spectator sport as well. Do you have a favourite tennis player?\""
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi, my name is Jeffrey. My favourite sport is tennis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b752d257-a024-45fa-806a-5d9411080a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Jeffrey. My favourite sport is tennis.\n",
      "AI: Hello Jeffrey, it's nice to meet you! Tennis is a great sport. Did you know that it originated in 12th century France as a game played with the palm of the hand? It wasn't until the 16th century that rackets were introduced. Today, tennis is played all over the world and is a popular spectator sport as well. Do you have a favourite tennis player?\n",
      "Human: My favourite player is Roger Federer.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Ah, Roger Federer is a great choice! He has won a record 20 Grand Slam singles titles and has been ranked world No. 1 in men's singles tennis by the Association of Tennis Professionals a record total of 310 weeks. He is known for his elegant playing style and his ability to play well on all surfaces. Have you ever seen him play in person?\""
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"My favourite player is Roger Federer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "6fa61b70-5089-4264-b568-f534eff7f3b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Jeffrey. My favourite sport is tennis.\n",
      "AI: Hello Jeffrey, it's nice to meet you! Tennis is a great sport. Did you know that it originated in 12th century France as a game played with the palm of the hand? It wasn't until the 16th century that rackets were introduced. Today, tennis is played all over the world and is a popular spectator sport as well. Do you have a favourite tennis player?\n",
      "Human: My favourite player is Roger Federer.\n",
      "AI: Ah, Roger Federer is a great choice! He has won a record 20 Grand Slam singles titles and has been ranked world No. 1 in men's singles tennis by the Association of Tennis Professionals a record total of 310 weeks. He is known for his elegant playing style and his ability to play well on all surfaces. Have you ever seen him play in person?\n",
      "Human: What is my name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Your name is Jeffrey, as you mentioned earlier.'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718983b1-caf3-4781-8f9e-b587d9ebfa4e",
   "metadata": {},
   "source": [
    "We see that we can have a conversation with AI, then after multiple inputs, ask it again what is my name. The AI remembers my name even though it was from a conversation from awhile back. Using this conversation chain with the defined memory object, we can now store the historical conversation.\n",
    "\n",
    "We can also print out the conversation up till now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "06ce979c-e0c1-462b-a938-46b13ede31a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi, my name is Jeffrey. My favourite sport is tennis.\n",
      "AI: Hello Jeffrey, it's nice to meet you! Tennis is a great sport. Did you know that it originated in 12th century France as a game played with the palm of the hand? It wasn't until the 16th century that rackets were introduced. Today, tennis is played all over the world and is a popular spectator sport as well. Do you have a favourite tennis player?\n",
      "Human: My favourite player is Roger Federer.\n",
      "AI: Ah, Roger Federer is a great choice! He has won a record 20 Grand Slam singles titles and has been ranked world No. 1 in men's singles tennis by the Association of Tennis Professionals a record total of 310 weeks. He is known for his elegant playing style and his ability to play well on all surfaces. Have you ever seen him play in person?\n",
      "Human: What is my name?\n",
      "AI: Your name is Jeffrey, as you mentioned earlier.\n"
     ]
    }
   ],
   "source": [
    "# see the stored history\n",
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cdb3da-14c3-480c-8148-600201175c59",
   "metadata": {},
   "source": [
    "We can also explicitly set a conversation as context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "45c42a56-6c7b-437c-9d91-a55a46ecce9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi\n",
      "AI: What's up\n"
     ]
    }
   ],
   "source": [
    "memory = ConversationBufferMemory()\n",
    "memory.save_context(\n",
    "    {\"input\": \"Hi\"}, \n",
    "    {\"output\": \"What's up\"})\n",
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31618de-cae8-4099-8d6d-025f73321dc4",
   "metadata": {},
   "source": [
    "### How do LLM store memory?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33c2ea5-ce78-47b4-b33f-e7add757726b",
   "metadata": {},
   "source": [
    "Large Language Models are stateless, meaning that each call to the API is independent. The memory that we see is actually from us providing the full conversation up to that point as the context. We saw earlier that the memory buffer had the entire conversation history stored. However, overtime as the conversation gets longer, the memory will eventually be too big and costly to keep passing in as input. LangChain provides many different ways to handle its memory. We have so far seen the most basic, ConversationBufferMemory() which just stores all the conversation up to that point as context. Let's explore other memories in LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273ecd4e-a07b-4dec-a710-5eaa9210491a",
   "metadata": {},
   "source": [
    "### ConversationBufferWindowMemory\n",
    "\n",
    "One way to manage storage of history is to only keep the last 'k' conversations in history. We can do this with the ConversationBufferWindowMemory, which has a parameter 'k'.\n",
    "\n",
    "For example, below we set k=1. We can see from the loaded memory variables that it has only stored the most recent conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "3b0c68e3-282b-41d9-abc6-03e21b2495bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: Yes, he is one of my favourite players.\\nAI: He has over 20 grand slams.'}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k=1) \n",
    "memory.save_context({\"input\": \"Hi, my name is Jeffrey and I like tennis\"},\n",
    "                    {\"output\": \"Hi Jeffrey, have you heard of Roger Federer?\"})\n",
    "memory.save_context({\"input\": \"Yes, he is one of my favourite players.\"},\n",
    "                    {\"output\": \"He has over 20 grand slams.\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f80f808-c22e-4c45-925b-be25ba7040f0",
   "metadata": {},
   "source": [
    "We can demonstrate this in a conversation as well, where we can see below that the AI can not remember what my name is, even though it was introduced in the beginning. This is because the window is only k=1, so it only stores the most recent conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a9fd0c9f-7e45-4a34-926c-cfc70d749aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.0, model=llm_model)\n",
    "memory = ConversationBufferWindowMemory(k=1)\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = memory,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "7c77844a-d7de-4fce-b656-41941984ec85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello Jeffrey, it's nice to meet you. My name is OpenAI. How can I assist you today?\""
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi, my name is Jeffrey.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "ffc38ff8-2251-44c4-b909-dd3adabe5fa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1 + 1 equals 2, as it is a basic arithmetic operation.'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is 1 + 1.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "0f482dbf-51fd-48a7-ba42-ccd97384e971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, I don't have access to that information. Could you please tell me your name?\""
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is my name.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1da68c6-0793-41d4-bd34-3e8993d616e7",
   "metadata": {},
   "source": [
    "### ConversationTokenBufferMemory\n",
    "\n",
    "Another way to store memory is to store a certain number of tokens (e.g. characters) in the history, rather than number of conversations. Conversations can vary in length, and some can be very long, so storing by tokens is a more consistent way of storing history from a memory perspective. Costs for API calls are usually also dependent on token length, so this method is more directly related to the cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "8d826089-7921-4af3-bffa-025c316ffc67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3365f35-ae8a-45c8-a638-28b9ff6ce9b5",
   "metadata": {},
   "source": [
    "Let's start by setting the max token limit to 100. In this example, we see that it stores only the most recent conversation. This is because it is under 100 tokens, however if we were to include the conversation earlier it would be over 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "6c4e4e19-c43d-46f3-84b8-58869520f67d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: My favourite player is Roger Federer.\\nAI: Ah, Roger Federer is a great choice! He has won a record 20 Grand Slam singles titles and has been ranked world No. 1 in men's singles tennis by the Association of Tennis Professionals a record total of 310 weeks. He is known for his elegant playing style and his ability to play well on all surfaces. Have you ever seen him play in person?\"}"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "from langchain.llms import OpenAI\n",
    "llm = ChatOpenAI(temperature=0.0, model=llm_model)\n",
    "\n",
    "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=100)\n",
    "memory.save_context({\"input\": \"Hi, my name is Jeffrey. My favourite sport is tennis.\"},\n",
    "                    {\"output\": \"Hello Jeffrey, it's nice to meet you! Tennis is a great sport. Did you know that it originated in 12th century France as a game played with the palm of the hand? It wasn't until the 16th century that rackets were introduced. Today, tennis is played all over the world and is a popular spectator sport as well. Do you have a favourite tennis player?\"})\n",
    "memory.save_context({\"input\": \"My favourite player is Roger Federer.\"},\n",
    "                    {\"output\": \"Ah, Roger Federer is a great choice! He has won a record 20 Grand Slam singles titles and has been ranked world No. 1 in men's singles tennis by the Association of Tennis Professionals a record total of 310 weeks. He is known for his elegant playing style and his ability to play well on all surfaces. Have you ever seen him play in person?\"})\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91c3d61-8d50-4c79-9e35-d716b334672a",
   "metadata": {},
   "source": [
    "If instead we set the max token limit to 10, we see that it does not store anything, becase the most recent conversation has over 10 tokens. It will only store a conversation if it is under the max token limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "db65d88d-7154-4919-9076-4fafe781551c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': ''}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=10)\n",
    "memory.save_context({\"input\": \"Hi, my name is Jeffrey. My favourite sport is tennis.\"},\n",
    "                    {\"output\": \"Hello Jeffrey, it's nice to meet you! Tennis is a great sport. Did you know that it originated in 12th century France as a game played with the palm of the hand? It wasn't until the 16th century that rackets were introduced. Today, tennis is played all over the world and is a popular spectator sport as well. Do you have a favourite tennis player?\"})\n",
    "memory.save_context({\"input\": \"My favourite player is Roger Federer.\"},\n",
    "                    {\"output\": \"Ah, Roger Federer is a great choice! He has won a record 20 Grand Slam singles titles and has been ranked world No. 1 in men's singles tennis by the Association of Tennis Professionals a record total of 310 weeks. He is known for his elegant playing style and his ability to play well on all surfaces. Have you ever seen him play in person?\"})\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9819cb2f-b35b-4e90-8c86-63c26d168c19",
   "metadata": {},
   "source": [
    "### ConversationSummaryBufferMemory\n",
    "\n",
    "When conversations get really long, both the window and token methods are not satisfactory, since they will start dropping more and more of the conversations if costs are to be maintained. A neat trick to get around this is to use the ConversationSummaryBufferMemory. This object similarly has a max_token_limit like before, however when the memory is past this max_token_limit, it will summarise all the conversations up to now using LLM, and then keep the summarised conversations as the context instead. It will then process new conversations as usual until the max_token_limit is reach once again, and it will summarise the conversations thus far again, and so on.\n",
    "\n",
    "Let's see an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "8de5f8b7-23d0-4f03-b97a-4179d0b79dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "# create a long string\n",
    "listing_description = \"\"\"\n",
    "Eva Building - Near New & Luxury Apartment with 2 Large Balconies\n",
    "Stylishly appointed this near-new three-bedroom apartment is perfectly located in the building of Eva Lane Cove. showcases a bright and versatile floor plan with spacious living and beautiful riverside views. Just footsteps to Hughes Park and a short stroll to city buses, cafes', shops and the bustling village also local schools.\n",
    "\n",
    "Features including:\n",
    "* Situated in sought-after location, enjoy parkside and riverside views\n",
    "* Generous 2 bedrooms plus a multi-function room\n",
    "* Large 2 balconies all with East aspects\n",
    "* Elegance 2-layer blackout curtains in the living area and bedrooms\n",
    "* Spacious interiors with a versatile open-plan living and dining area\n",
    "* Island modern kitchen with 'Millie' appliances, gas cooking and dishwasher\n",
    "* Three bedrooms all with built-in, the main bedroom with ensuite\n",
    "* Sparkling bathroom with floor-to-ceiling tiles\n",
    "* Ducting Air conditioning\n",
    "* Video intercom and internal laundry.\n",
    "* Secure one car space and storage\n",
    "\n",
    "Outgoings:\n",
    "Strata levy:$1208.60 pq\n",
    "Council rate: $359.00 pq\n",
    "Water: $158.45 pq approx.\n",
    "\"\"\"\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=200)\n",
    "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})\n",
    "memory.save_context({\"input\": \"Give me a property listing,\"}, \n",
    "                    {\"output\": f\"{listing_description}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "480191cf-9137-4ac8-acb9-29558398efda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'System: The human greets the AI and asks for a property listing. The AI provides a detailed description of a luxury apartment with riverside views, spacious living areas, and modern amenities. The apartment is located near parks, cafes, shops, and schools. The outgoings include strata levy, council rate, and water fees.'}"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b03ff7-32f4-41ac-8bf6-e15bbb39d62f",
   "metadata": {},
   "source": [
    "In above example, we give the memory a long property listing. Since it is more than 200 tokens, it is summarised in a few short sentences and this then becomes its context/memory.\n",
    "\n",
    "If we then prompt this further, the AI will reply, having the summarised context. The new AI response is now also part of the memory - with the latest response not being summarised since in total it is still under 200 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a57c1f0c-89bc-4919-97ba-3c41e875ab65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: The human greets the AI and asks for a property listing. The AI provides a detailed description of a luxury apartment with riverside views, spacious living areas, and modern amenities. The apartment is located near parks, cafes, shops, and schools. The outgoings include strata levy, council rate, and water fees.\n",
      "Human: Do you think it is a good property?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Based on the features and location of the property, it appears to be a high-quality and desirable option for those seeking luxury living with convenient access to local amenities. However, whether or not it is a good property ultimately depends on individual preferences and needs.'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = memory,\n",
    "    verbose=True\n",
    ")\n",
    "conversation.predict(input=\"Do you think it is a good property?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "c1218fc1-5912-4e76-8b2a-97a1b2c46653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'System: The human greets the AI and asks for a property listing. The AI provides a detailed description of a luxury apartment with riverside views, spacious living areas, and modern amenities. The apartment is located near parks, cafes, shops, and schools. The outgoings include strata levy, council rate, and water fees.\\nHuman: Do you think it is a good property?\\nAI: Based on the features and location of the property, it appears to be a high-quality and desirable option for those seeking luxury living with convenient access to local amenities. However, whether or not it is a good property ultimately depends on individual preferences and needs.'}"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f7a72f-4d57-4c9b-aa48-25e4e9064fcc",
   "metadata": {},
   "source": [
    "### Other memory methods\n",
    "\n",
    "There are many other memory types, and you are encouraged to have a look at what other options are there. These include:\n",
    "- Vector data memory - stores text in a vector database and retrives the most relevant blocks of text\n",
    "- Entity memories - remembers details about specific entities\n",
    "- and more...\n",
    "\n",
    "We can use multiple memories at one time, so go experiment!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b61afeb-b5d2-48bc-86ad-0d2c7e223c03",
   "metadata": {},
   "source": [
    "## Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fd44bf-2d85-4196-8532-4286ab260c4a",
   "metadata": {},
   "source": [
    "### What is a chain?\n",
    "\n",
    "We will now explore the key building block of LangChain, the chain. The chain combines LLM together with a prompt, and we can think of this as a building block. We combine these chains together to carry out a sequence of tasks. Previously, we looked at a specific use case of LangChain, namely ConversationChain, which we used to have a conversation with AI. We will now take a step back, and look at the most basic building block of LangChain, the **LLMChain**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e966d8-5fdc-42b1-a4cb-805d0646b969",
   "metadata": {},
   "source": [
    "### LLMChain\n",
    "\n",
    "We will start with exploring a basic but important chain, the LLMChain. This is the most common type of chain. It consists of:\n",
    "- PromptTemplate\n",
    "- A model (e.g. LLM, ChatModel)\n",
    "- Optional output parser\n",
    "\n",
    "The chain takes multiple input variables, uses the PromptTemplate to format them into a prompt. This gets fed into the model. The optional output parser will then parse the output of the model into a defined final format.\n",
    "\n",
    "Let us show a simple application of the LLMChain, by using the ChatPromptTemplate, and ChatOpenAI model (which we saw previously), to have an AI respond to our questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "9b662689-709b-4754-a8d2-d8e6518accbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Property Finder\" or \"Listing Compare\" could be good names for a product that compares property listings.'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.9, model=llm_model)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"What is the best name to describe \\\n",
    "    a product that {product}?\"\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "product = \"compares property listings\"\n",
    "chain.run(product)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afefb80-6142-4c14-9202-598e906c3b1f",
   "metadata": {},
   "source": [
    "Here we first choose the ChatOpenAI end point, which is using the GPT-3.5 model.\n",
    "\n",
    "We define a prompt, which simply asks \"What is the best name to descrbie a product that {}\". The input to this prompt is a decription of what the product will do.\n",
    "\n",
    "We then define a LLMChain, which will take this prompt, and use the GPT-3.5 model. This chain is now ready to take inputs, and produce and output.\n",
    "\n",
    "We then feed in \"compares property listings\" as input, this is added into the prompt template, passed to GPT-3.5 model, and then output produced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6af2db7-468f-4f44-9779-0976864727d4",
   "metadata": {},
   "source": [
    "### SimpleSequentialChain\n",
    "\n",
    "We've seen how a LLMChain works, and we can think of these as building blocks to build more complicated use cases. The real value from chains is when we combine these building blocks together. Let's look at how we could do this using SimpleSequentialChain.\n",
    "\n",
    "SimpleSequentialChains allow us to take the output from one call and use it as the input to another. In the case of a simple chain, each step has a singular input/output, and the output of one step is the input to the next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c2f803-936c-4118-8fe3-595fa8be5f8a",
   "metadata": {},
   "source": [
    "First, let us define a simple chain using LLMChain as before, and call this chain_one. This chain like before, will take as input a description of what a product does, and the output will be a name for this product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "4014ba0c-e764-42a0-968a-27b27fc22fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "e7ed9d02-bdd8-4b19-b351-5105235d9e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.9, model=llm_model)\n",
    "\n",
    "# prompt template 1\n",
    "first_prompt = ChatPromptTemplate.from_template(\n",
    "    \"What is the best name to describe \\\n",
    "    a product that {product_function}?\"\n",
    ")\n",
    "\n",
    "# Chain 1\n",
    "chain_one = LLMChain(llm=llm, prompt=first_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da3aa11-93d6-4bb1-a8e4-90ceb7118724",
   "metadata": {},
   "source": [
    "We'll now define a second chain, also using LLMChain, and call this chain_two. This will take as input the product name outputted from chain 1, and generate a 20 word description about this product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "fefcafad-0a51-4841-99b9-3a5933e9000c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt template 2\n",
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Write a 20 words description for the following \\\n",
    "    product:{product_description}\"\n",
    ")\n",
    "# chain 2\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341a2d26-3aec-4e32-90ae-e9ee76ad6cbe",
   "metadata": {},
   "source": [
    "Finally, let us wrap all this in a SimpleSequentialChain. This takes as its argument, a list of chains in sequential order. In this case, we pass in chain_one and chain_two as the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "59140eff-4564-47fd-93fb-ccf71ca24490",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_simple_chain = SimpleSequentialChain(\n",
    "    chains=[chain_one, chain_two],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f7c792-7573-489e-ab4c-49e235dc3f8f",
   "metadata": {},
   "source": [
    "We then call this sequential chain, giving as input a description of what the product does as \"compares property listings\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "ae55e77a-53ce-4497-b3ae-3d420638d970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m\"Listings Comparison Tool\" or \"Property Comparison Engine\" are some good names to describe a product that compares property listings.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mA tool that enables property buyers to easily compare different listings, making the search process quicker and more efficient.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'A tool that enables property buyers to easily compare different listings, making the search process quicker and more efficient.'"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_function = \"compares property listings\"\n",
    "overall_simple_chain.run(product_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1e93da-4fdb-4de7-b8da-4f4d6a8dfaaf",
   "metadata": {},
   "source": [
    "### SequentialChain\n",
    "\n",
    "In the previous example, the SimpleSequentialChain only allowed singular inputs and outputs that are passed in a linear fashion, and we only retained the final output from the chain. What if we have multiple inputs? Or what if we wanted to retain multiple outputs that are generated passed along the chain? We can do this with SequentialChain. We will illustrate this with an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d429654-a72f-4b2b-be95-7aa7b41a5cab",
   "metadata": {},
   "source": [
    "We first set the first LLMChain as before. The only difference is this time, we also specify the output_key in the LLMChain. This is required so that we can know how to refer to this output later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "ce2c3b2e-3aef-4f6a-b822-f01e27524b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "f296e962-db46-4867-96fe-16fdce45f06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.9, model=llm_model)\n",
    "\n",
    "# prompt template 1\n",
    "first_prompt = ChatPromptTemplate.from_template(\n",
    "    \"What is the best name to describe \\\n",
    "    a product that {product_function}?\"\n",
    ")\n",
    "\n",
    "# Chain 1\n",
    "chain_one = LLMChain(llm=llm, prompt=first_prompt, output_key=\"product_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03909ff-6891-448f-9b8f-d9fe279df08a",
   "metadata": {},
   "source": [
    "Similarly, for the second LLMChain, the only difference is the output_key which refers to the output from the second chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "8611c120-76f0-458e-85a8-9e82748e0721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt template 2\n",
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Write a 20 words description for the following \\\n",
    "    product:{product_name}\"\n",
    ")\n",
    "# chain 2\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt, output_key=\"product_description\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64feb18a-205d-4e15-8e38-b8b5e75dcca3",
   "metadata": {},
   "source": [
    "Finally, we wrap this in a SequentialChain. The key difference from a SimpleSequentialChain is the addition of the input_variables and output_variables. Here, we are clearly specifying that this chain expects one input, which is the description of the product, and it will output from the product name (output from the first LLMCHain) and the 20 word product description (output from the second LLMCHain). This is why the output_key earlier was important, since the SequentialChain knows which output based off the output_keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "a53bd5f4-6cd7-4ac9-b76b-ee4dea79c821",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_chain = SequentialChain(\n",
    "    chains=[chain_one, chain_two],\n",
    "    input_variables=[\"product_function\"],\n",
    "    output_variables=[\"product_name\",\"product_description\"],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f62c07-fcc1-4c1d-a935-7de7dbfef719",
   "metadata": {},
   "source": [
    "When we run this chain, we can now see that we have stored all the inputs and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "631729e6-7515-43d4-96cf-16359613f48c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'product_function': 'compares property listings',\n",
       " 'product_name': '\"Listings Comparator\" or \"Property Search Comparator\"',\n",
       " 'product_description': 'Our Listings/Property Search Comparator is a tool that helps you compare and find the best real estate listings based on your preferences.'}"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_function = \"compares property listings\"\n",
    "overall_chain(product_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171e2c0d-f605-4c30-abba-b6cc15476aae",
   "metadata": {},
   "source": [
    "### RouterChain\n",
    "\n",
    "In many cases, we need something more flexible than a sequential chain. Suppose we have questions of different natures, e.g. english questions, maths questions and computer science questions. In real life, we would ask an expert in the field rather than as a english writer a maths questions! We can model this process of selecting the right person using a RouterChain.\n",
    "\n",
    "A RouterChain lets us create a chain that dynamically selects the next chain based of a given input. This allows us to define different chains with different contexts (very important for LLM), and then depending on the input, it will direct to the most relevant chain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7856b8b8-becc-4d06-b212-e34204dc85af",
   "metadata": {},
   "source": [
    "Let us see this in an example. We start by defining different templates, which we can think of as the context for the different people (which will end up being different chains)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "0b2b4e1b-de94-49ba-87de-0d9a42e406a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_template = \"\"\"You are a very smart english writer. \\\n",
    "You are very creative, and love reading and writing stories.\\\n",
    "You have good knowledge regarding all different kinds of books \\\n",
    "including fiction and non-fiction, and are good at presenting \\\n",
    "interpretations and open to different ideas.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "\n",
    "math_template = \"\"\"You are a very good mathematician. \\\n",
    "You are great at answering math questions. \\\n",
    "You are so good because you are able to break down \\\n",
    "hard problems into their component parts, \n",
    "answer the component parts, and then put them together\\\n",
    "to answer the broader question.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "\n",
    "computerscience_template = \"\"\" You are a successful computer scientist.\\\n",
    "You have a passion for creativity, collaboration,\\\n",
    "forward-thinking, confidence, strong problem-solving capabilities,\\\n",
    "understanding of theories and algorithms, and excellent communication \\\n",
    "skills. You are great at answering coding questions. \\\n",
    "You are so good because you know how to solve a problem by \\\n",
    "describing the solution in imperative steps \\\n",
    "that a machine can easily interpret and you know how to \\\n",
    "choose a solution that has a good balance between \\\n",
    "time complexity and space complexity. \n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50852822-b519-464b-b0ee-67e4fb94273a",
   "metadata": {},
   "source": [
    "We put this into a list, with each item being the prompt template that will be fed into each LLMChain. Note that each item here is uding the prompt_template we defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "92bf5d26-7814-4b59-a46e-62e92b505069",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_infos = [\n",
    "    {\n",
    "        \"name\": \"english\", \n",
    "        \"description\": \"Good for answering questions about english\", \n",
    "        \"prompt_template\": english_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"math\", \n",
    "        \"description\": \"Good for answering math questions\", \n",
    "        \"prompt_template\": math_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"computer science\", \n",
    "        \"description\": \"Good for answering computer science questions\", \n",
    "        \"prompt_template\": computerscience_template\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910ff6e1-87fd-46d2-8bc7-e4d5a85ef469",
   "metadata": {},
   "source": [
    "We then loop through the list of prompts, and create each LLMChain for each person. For each LLMChain, we use the GPT-3.5 model, and define the prompt based off the respective prompt templates.\n",
    "\n",
    "We also create a destintation_str which will be useful later when we need to define our router_template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "0074d185-1ab9-47b2-b04e-887bdb714178",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.router import MultiPromptChain\n",
    "from langchain.chains.router.llm_router import LLMRouterChain,RouterOutputParser\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "158396ba-1e54-4712-83ad-99aa21ab9f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0, model=llm_model)\n",
    "\n",
    "\n",
    "destination_chains = {}\n",
    "for p_info in prompt_infos:\n",
    "    name = p_info[\"name\"]\n",
    "    prompt_template = p_info[\"prompt_template\"]\n",
    "    prompt = ChatPromptTemplate.from_template(template=prompt_template)\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    destination_chains[name] = chain  \n",
    "    \n",
    "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
    "destinations_str = \"\\n\".join(destinations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bae5497-3de7-4d82-9a47-9a6b750be5bd",
   "metadata": {},
   "source": [
    "We also define a default prompt when none of the destinations fit well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "c8f9faed-eb28-4e92-b1e0-a3476381c01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
    "default_chain = LLMChain(llm=llm, prompt=default_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9a4903-bd95-4e4b-be1d-511ef8d23bbb",
   "metadata": {},
   "source": [
    "We are finally ready to bring this all together. The LLMRouterChain requires a prompt template that includes description on how to format the output, and what destinations are available etc. We use the router template below.\n",
    "\n",
    "Note that it clearly talks about needing to route to the most relevant LLMChain, and the input and output it expects. The specific details of the destinations, input and output have been abstracted away by out templates we defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "d382766d-b699-4978-9771-2f47c53337fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"Given a raw text input to a \\\n",
    "language model select the model prompt best suited for the input. \\\n",
    "You will be given the names of the available prompts and a \\\n",
    "description of what the prompt is best suited for. \\\n",
    "You may also revise the original input if you think that revising\\\n",
    "it will ultimately lead to a better response from the language model.\n",
    "\n",
    "<< FORMATTING >>\n",
    "Return a markdown code snippet with a JSON object formatted to look like:\n",
    "```json\n",
    "{{{{\n",
    "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
    "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
    "}}}}\n",
    "```\n",
    "\n",
    "REMEMBER: \"destination\" MUST be one of the candidate prompt \\\n",
    "names specified below OR it can be \"DEFAULT\" if the input is not\\\n",
    "well suited for any of the candidate prompts.\n",
    "REMEMBER: \"next_inputs\" can just be the original input \\\n",
    "if you don't think any modifications are needed.\n",
    "\n",
    "<< CANDIDATE PROMPTS >>\n",
    "{destinations}\n",
    "\n",
    "<< INPUT >>\n",
    "{{input}}\n",
    "\n",
    "<< OUTPUT (remember to include the ```json)>>\"\"\"\n",
    "\n",
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n",
    "    destinations=destinations_str\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce30882-07d4-4672-87c7-102348185512",
   "metadata": {},
   "source": [
    "We now use this PromptTemplate as a parameters to the LLMRouterChain. Note that the PromptTemplate here has an output_parse which is RouterOutputParser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "b7910361-b642-4565-9540-2cabd8546ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "router_prompt = PromptTemplate(\n",
    "    template=router_template,\n",
    "    input_variables=[\"input\"],\n",
    "    output_parser=RouterOutputParser(),\n",
    ")\n",
    "\n",
    "router_chain = LLMRouterChain.from_llm(llm, router_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "67fd56c9-5783-4c44-a22b-1fe93cbb34de",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = MultiPromptChain(router_chain=router_chain, \n",
    "                         destination_chains=destination_chains, \n",
    "                         default_chain=default_chain, verbose=True\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651db328-1679-4560-b38f-f6fc3004033b",
   "metadata": {},
   "source": [
    "We can now run this chain, just by giving it a question. It will decide in the backend which destination it should go to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "e19df1ed-f548-4297-8708-fecf1c06ebb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeffrey.lee/.pyenv/versions/3.9.13/lib/python3.9/site-packages/langchain/chains/llm.py:280: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "math: {'input': 'What is 1 + 1'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'As an AI language model, I can answer that the sum of 1 and 1 is 2.'"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"What is 1 + 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "2d6184a1-e457-4a2b-ba72-3b7430dc14e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeffrey.lee/.pyenv/versions/3.9.13/lib/python3.9/site-packages/langchain/chains/llm.py:280: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None: {'input': 'What fiction book was very influential in the 20th century and why?'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'One fiction book that was very influential in the 20th century is \"1984\" by George Orwell. This book is a dystopian novel that portrays a totalitarian government that controls every aspect of its citizens\\' lives. It was published in 1949 and has since become a classic of modern literature.\\n\\nThe book\\'s influence can be seen in its impact on political and social discourse. The term \"Orwellian\" has become synonymous with oppressive government control and surveillance. The book\\'s themes of censorship, propaganda, and the manipulation of language have also become important topics in discussions of politics and media.\\n\\nAdditionally, \"1984\" has influenced other works of literature, film, and television. Its ideas have been adapted and referenced in countless works, from \"The Handmaid\\'s Tale\" to \"Black Mirror.\"\\n\\nOverall, \"1984\" remains a powerful and relevant work of fiction that continues to shape our understanding of politics, society, and the human condition.'"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"What fiction book was very influential in the 20th century and why?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6febc0-6fde-4eab-b862-9afb77bba825",
   "metadata": {},
   "source": [
    "### Other chains to explore\n",
    "\n",
    "There are many other chains to explore. Some are below:\n",
    "- EmbeddingRouterChain - similar to RouterChain, but rather than using LLM to decide which chain to go to, uses embeddings and similarities to route between destination chains\n",
    "- TransformChain - It provides a way to transform input values to output values using a specified transform function. For example, if we want to filter input text to only the first 3 sentences, we can use a TransformChain first, before then passing it to another chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0068c1de-e85d-4adc-82dd-0beffa9c1cdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
